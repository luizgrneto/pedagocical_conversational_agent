{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ecfcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgrne\\OneDrive\\Documents\\AULAS UFG\\TCC\\Pedagogical_Conversational_Agent\\pca_agent_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from src.text_utils import extract_text_from_pdf, text_splitter\n",
    "from src.utils import list_files_per_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a7618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_file_list = list_files_per_subject('literature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b578a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciating the ChromaDB client\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\" )\n",
    "\n",
    "# tenta obter ou criar a coleção\n",
    "try:\n",
    "    collection = client.get_collection(\"meus_docs\")\n",
    "except Exception:\n",
    "    collection = client.create_collection(\"meus_docs\")\n",
    "\n",
    "# Loading embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e948dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from:  literature/teste/teste.pdf\n",
      "Extracted 17019 characters.\n",
      "Split into 24 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n",
      "Extracting text from:  literature/mlops/MLOps Now - The MLOps Platform_ Revolutionising Machine Learning Efficiency.pdf\n",
      "Extracted 17019 characters.\n",
      "Split into 24 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n",
      "Extracting text from:  literature/mlops/Hidden_technical_debt_in_machine_learning_systems.pdf\n",
      "Extracted 36224 characters.\n",
      "Split into 47 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 31.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n",
      "Extracting text from:  literature/mlops/MLOps Now - MLOps Best Practices and Challenges.pdf\n",
      "Extracted 9387 characters.\n",
      "Split into 13 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n",
      "Extracting text from:  literature/mlops/MLOps Now - ML Engineer vs Data Scientist.pdf\n",
      "Extracted 5187 characters.\n",
      "Split into 7 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n",
      "Extracting text from:  literature/mlops/Introduction to MLOps _ Paperspace Blog.pdf\n",
      "Extracted 16110 characters.\n",
      "Split into 23 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n",
      "Extracting text from:  literature/mlops/What is MLOps_ - Machine Learning Operations Explained - AWS.pdf\n",
      "Extracted 13901 characters.\n",
      "Split into 19 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n",
      "Extracting text from:  literature/mlops/MLOps Now - What is MLOps_ Demystifying Machine Learning Operations.pdf\n",
      "Extracted 16730 characters.\n",
      "Split into 23 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n",
      "Extracting text from:  literature/mlops/practitioners_guide_to_mlops_whitepaper.pdf\n",
      "Extracted 64873 characters.\n",
      "Split into 90 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 31.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from:  literature/mlops/Operationalizing_Machine_Learning_An_Interview_Study.pdf\n",
      "Extracted 110095 characters.\n",
      "Split into 144 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:00<00:00, 27.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n",
      "Extracting text from:  literature/mlops/What is MLOps.txt\n",
      "Extracted 28925 characters.\n",
      "Split into 38 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 56.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n",
      "Extracting text from:  literature/mlops/MLOps_ Continuous delivery and automation pipelines in machine learning  _  Cloud Architecture Center  _  Google Cloud Documentation.pdf\n",
      "Extracted 36653 characters.\n",
      "Split into 49 chunks.\n",
      "Embedding chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 35.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in ChromaDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file in complete_file_list:\n",
    "    print(\"Extracting text from: \", file['file_path'])\n",
    "    text = extract_text_from_pdf(file['file_path'])\n",
    "    print(\"Extracted\", len(text), \"characters.\")\n",
    "\n",
    "    splitted_text = text_splitter(text, \n",
    "                                  chunk_size=1000, \n",
    "                                  chunk_overlap=200)\n",
    "    print(\"Split into\", len(splitted_text), \"chunks.\")\n",
    "\n",
    "    print(\"Embedding chunks...\")\n",
    "    embeddings = model.encode(splitted_text, show_progress_bar=True)\n",
    "\n",
    "    print(\"Storing in ChromaDB...\")\n",
    "    collection.add(\n",
    "        ids=[f\"chunk_{i}\" for i in range(len(splitted_text))],\n",
    "        documents=splitted_text,\n",
    "        embeddings=embeddings.tolist(),\n",
    "        metadatas=[{\"source\": file[\"dirname\"], \"page\": i} for i in range(len(splitted_text))]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca_agent_env (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
