[
  {
    "input": "O que é MLOps?",
    "expected_output": "MLOps é a disciplina que combina práticas de Machine Learning, DevOps e engenharia de software para gerenciar o ciclo de vida completo de modelos, desde desenvolvimento até monitoramento e manutenção contínua.",
    "actual_output": "Olá!\n\nDe acordo com o contexto fornecido pela AWS, MLOps (Machine Learning Operations) é um conjunto de práticas específicas para projetos de aprendizado de máquina. Ele inclui gerenciamento de dados, desenvolvimento automatizado de modelos, geração de código, treinamento e reentrenamento de modelos, desenvolvimento contínuo de modelos, deployment e monitoramento de modelos.\n\nMLOps é um conjunto de processos padronizados e capacidades tecnológicas para construir, implantar e operacionalizar sistemas de aprendizado de máquina rapidamente e confiavelmente. Ele apoia o desenvolvimento e a implantação de ML (Machine Learning) da mesma forma que DevOps e DataOps apoiam o engenharia de aplicativos e engenharia de dados.\n\nNenhuma informação disponível no contexto.",
    "retrieval_context": [
      "8/9\n11/11/25, 8:50 PM\nWhat is MLOps? - Machine Learning Operations Explained - AWS\nCreate an AWS account\nLearn\nResources\nDevelopers\nHelp\nEnglish\nAmazon is an Equal Opportunity Employer: Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.\nBack to top\nPrivacy\nSite terms\nCookie Preferences\n© 2025, Amazon Web Services, Inc. or its aﬃliates. All rights reserved.\nhttps://aws.amazon.com/what-is/mlops/\n9/9",
      "System administrators, data science teams, and other business units collaborate and communicate to foster a common understanding of how production models are created and maintained, much the same as DevOps does for software. DevOps is a proven practice that can provide rapid development life cycles, increase development velocity, improve code quality through proper testing, and help achieve faster time to market.\nWhy do we need MLOps?\nLong-term value and reduced risks. MLOps helps organizations generate long-term value while reducing the risks that are associated with data science, machine learning, and AI initiatives.",
      "deployed model prediction service from live data. This stage output is a trigger to run the pipeline or a new\nexperiment cycle.\nhttps://aws.amazon.com/what-is/mlops/\n6/9\n11/11/25, 8:50 PM\nWhat is MLOps? - Machine Learning Operations Explained - AWS\nWhat is the diﬀerence between MLOps and DevOps?\nMLOps and DevOps are both practices that aim to improve processes where you develop, deploy, and\nmonitor software applications.\nDevOps aims to bridge the gap between development and operations teams. DevOps helps ensure that code\nchanges are automatically tested, integrated, and deployed to production eﬃciently and reliably. It\npromotes a culture of collaboration to achieve faster release cycles, improved application quality, and more\neﬃcient use of resources.\nMLOps, on the other hand, is a set of best practices speciﬁcally designed for machine learning projects. While\nit can be relatively straightforward to deploy and integrate traditional software, ML models present unique",
      "MLOps encompasses more than model construction and design. It includes data management, automated model development, code generation, model training and retraining, continuous model development, deployment, and model monitoring. Incorporating DevOps ideas into machine learning offers a shorter development cycle, improved quality control, and the ability to adapt to changing business needs.",
      "operations (the Ops element). It advocates formalizing and (when beneficial) automating critical steps of ML system\nconstruction. MLOps provides a set of standardized processes and technology capabilities for building, deploying,\nand operationalizing ML systems rapidly and reliably.\nMLOps supports ML development and deployment in the way that DevOps and DataOps support application engi-\nneering and data engineering (analytics). The difference is that when you deploy a web service, you care about resil-\nience, queries per second, load balancing, and so on. When you deploy an ML model, you also need to worry about\nchanges in the data, changes in the model, users trying to game the system, and so on. This is what MLOps is about."
    ]
  },
  {
    "input": "Qual a diferença entre MlOps e Data Scientist?",
    "expected_output": "A diferença entre um Data Scientist e um ML Engineer é que o primeiro se concentra em desenvolver modelos de aprendizado automático, enquanto o segundo se concentra na engenharia da construção e implantação desses modelos.",
    "actual_output": "Olá!\n\nA pergunta é sobre a diferença entre MLOps e Data Scientist. Vou ajudar a esclarecer isso para você.\n\nMLOps (Machine Learning Operations) é um termo que se refere à cultura e prática de engenharia de ML que visa unificar o desenvolvimento (Dev) e operação (Ops) de sistemas de aprendizado de máquina. Isso significa que MLOps é uma abordagem que busca automatizar e monitorar todos os passos do ciclo de vida de um sistema de aprendizado de máquina, desde a integração até a implantação e gerenciamento da infraestrutura.\n\nJá o Data Scientist é um profissional que se especializa em extrair insights valiosas a partir de grandes volumes de dados. Eles têm uma sólida formação em matemática, estatística e programação, e trabalham para analisar e interpretar dados para resolver problemas complexos e apoiar tomadas de decisão melhoradas dentro de uma organização.\n\nEm resumo, MLOps é um termo que se refere à prática de engenharia de ML, enquanto Data Scientist é um profissional que se especializa em trabalhar com dados. Embora haja some similarities between the two (e.g., both involve working with data), they have distinct roles and responsibilities.\n\nEspero que isso tenha ajudado a esclarecer a diferença entre MLOps e Data Scientist!",
    "retrieval_context": [
      "1/4\n11/11/25, 10:05 PM\nMLOps Now - ML Engineer vs Data Scientist\nshare some similarities, they also have important differences that set them\napart. In this article, we’ll explore the distinctions between ML Engineer vs\nData Scientist, and discuss how each of these professionals contributes to\nthe machine learning ecosystem.\nWhat is a Data Scientist?\nA Data Scientist is an expert in extracting valuable insights from large\nvolumes of data. With a strong background in mathematics, statistics, and\nprogramming, Data Scientists analyze and interpret data to solve complex\nproblems and support better decision-making within an organization. They\nwork closely with business stakeholders to understand their objectives and\ndevelop machine learning models to help achieve those goals.",
      "MLOps Now - What is MLOps? Demystifying Machine Learning Operations\nTo understand MLOps, it’s essential to be familiar with the development\nlifecycle of data science projects. A typical data science project consists of\nseveral stages:\n1. Data acquisition: Obtaining raw data from various sources, such as\ndatabases, sensors, or external APIs.\n2. Data preprocessing: Cleaning, transforming, and structuring the data\nto prepare it for analysis.\n3. Feature engineering: Selecting the most relevant data attributes, or\n“features,” and converting them into a suitable format for ML\nalgorithms.\n4. Model training: Applying ML algorithms to the preprocessed data to\ncreate a predictive model.\n5. Model evaluation: Assessing the performance of the model and\nmaking adjustments to improve its accuracy.",
      "Figure 1. Elements for ML systems. Adapted from Hidden Technical Debt in Machine\nLearning Systems (https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf).\nThe preceding diagram displays the following system components:\nConfiguration\nAutomation\nData collection\nData verification\nTesting and debugging\nResource management\nModel analysis\nProcess and metadata management\nServing infrastructure\nMonitoring\nTo develop and operate complex systems like these, you can apply DevOps principles to ML\nsystems (MLOps). This document covers concepts to consider when setting up an MLOps environment for your data science practices, such as CI, CD, and CT in ML.\nThe following topics are discussed:",
      "Rapid advances in different ML research fields (such as computer vision, natural language understanding, generative AI, and recommendations AI systems).\nTherefore, many businesses are investing in their data science teams and ML capabilities\nto develop predictive models that can deliver business value to their users.\nThis document is for data scientists and ML engineers who want to apply DevOps\n(https://cloud.google.com/devops/) principles to ML systems (MLOps). MLOps is an ML engineering culture and practice that aims at unifying ML system development (Dev) and\nML system operation (Ops). Practicing MLOps means that you advocate for automation\nand monitoring at all steps of ML system construction, including integration, testing, releasing, deployment and infrastructure management.\nData scientists can implement and train an ML model with predictive performance on an",
      "Large datasets\nInexpensive on-demand compute resources\nSpecialized accelerators for ML on various cloud platforms\nRapid advances in different ML research fields (such as computer vision, natural language understanding, generative AI, and recommendations AI systems).\nTherefore, many businesses are investing in their data science teams and ML capabilities\nto develop predictive models that can deliver business value to their users.\nThis document is for data scientists and ML engineers who want to apply DevOps\n(https://cloud.google.com/devops/) principles to ML systems (MLOps). MLOps is an ML engineering culture and practice that aims at unifying ML system development (Dev) and\nML system operation (Ops). Practicing MLOps means that you advocate for automation"
    ]
  },
  {
    "input": "Qual a diferença entre MLOps e DevOps?",
    "expected_output": "DevOps foca em desenvolvimento e operação de software tradicional, enquanto MLOps lida também com dados, treinamento de modelos, versionamento de experimentos e monitoramento de métricas de inferência e drift.",
    "actual_output": "Olá!\n\nA pergunta é sobre a diferença entre MLOps (Machine Learning Operations) e DevOps. De acordo com o contexto, podemos ver que ambos compartilham objetivos semelhantes: unificar desenvolvimento de software e operações.\n\nDevOps é uma prática que combina desenvolvimento (Dev) e operações (Ops), visando reduzir o tempo entre mudanças de código e implantação. Isso é alcançado mediante a automação de processos como integração contínua, entrega contínua e deployment contínuo.\n\nJá MLOps é uma prática que combina desenvolvimento (Dev) e operações (Ops) específicas para sistemas de aprendizado de máquina (ML). Isso inclui gerenciamento de dados, desenvolvimento automatizado de modelos, geração de código, treinamento e re-treinamento de modelos, deployment contínuo e monitoramento de modelos.\n\nA principal diferença entre MLOps e DevOps é que MLOps se concentra especificamente em sistemas de aprendizado de máquina, enquanto DevOps é uma prática mais geral para desenvolvimento e operações de software. Portanto, podemos dizer que MLOps é um subconjunto de DevOps, aplicado ao contexto específico de aprendizado de máquina.\n\nEspero que isso tenha ajudado a esclarecer a diferença entre MLOps e DevOps!",
    "retrieval_context": [
      "our other blog post.\nMLOps and DevOps\nThe DevOps Paradigm\nDevOps is a combination of development (Dev) and operations (Ops)\npractices, aimed at unifying software development and IT operations. The\nprimary goal of DevOps is to reduce the time taken from code changes to\noperational deployment. This is achieved by embracing automation for\nhttps://mlopsnow.com/blog/what-is-mlops/\n3/11\n11/11/25, 10:05 PM\nMLOps Now - What is MLOps? Demystifying Machine Learning Operations\ntasks like continuous integration (CI), continuous delivery (CD), and\ncontinuous deployment.\nAutomation is a fundamental aspect of DevOps, providing consistency,\nspeed, and reliability. It includes automating processes such as building,\ntesting and deploying code, supporting infrastructure management and",
      "systems (MLOps). This document covers concepts to consider when setting up an MLOps environment for your data science practices, such as CI, CD, and CT in ML.\nThe following topics are discussed:\nhttps://docs.cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning\n2/18\n11/13/25, 11:39 PM\nMLOps: Continuous delivery and automation pipelines in machine learning | Cloud Architecture Center | Google Cloud Do…\nDevOps versus MLOps\nSteps for developing ML models\nMLOps maturity levels\nMlOps for generative AI\nDevOps versus MLOps\nDevOps (https://cloud.google.com/devops/) is a popular practice in developing and operating\nlarge-scale software systems. This practice provides benefits such as shortening the\ndevelopment cycles, increasing deployment velocity, and dependable releases. To achieve\nthese benefits, you introduce two concepts in the software system development:\nContinuous integration (CI) (https://en.wikipedia.org/wiki/Continuous_integration)",
      "MLOps encompasses more than model construction and design. It includes data management, automated model development, code generation, model training and retraining, continuous model development, deployment, and model monitoring. Incorporating DevOps ideas into machine learning offers a shorter development cycle, improved quality control, and the ability to adapt to changing business needs.",
      "System administrators, data science teams, and other business units collaborate and communicate to foster a common understanding of how production models are created and maintained, much the same as DevOps does for software. DevOps is a proven practice that can provide rapid development life cycles, increase development velocity, improve code quality through proper testing, and help achieve faster time to market.\nWhy do we need MLOps?\nLong-term value and reduced risks. MLOps helps organizations generate long-term value while reducing the risks that are associated with data science, machine learning, and AI initiatives.",
      "Rapid advances in different ML research fields (such as computer vision, natural language understanding, generative AI, and recommendations AI systems).\nTherefore, many businesses are investing in their data science teams and ML capabilities\nto develop predictive models that can deliver business value to their users.\nThis document is for data scientists and ML engineers who want to apply DevOps\n(https://cloud.google.com/devops/) principles to ML systems (MLOps). MLOps is an ML engineering culture and practice that aims at unifying ML system development (Dev) and\nML system operation (Ops). Practicing MLOps means that you advocate for automation\nand monitoring at all steps of ML system construction, including integration, testing, releasing, deployment and infrastructure management.\nData scientists can implement and train an ML model with predictive performance on an"
    ]
  },
  {
    "input": "O que é drift em modelos de machine learning?",
    "expected_output": "Drift é a mudança estatística no comportamento dos dados ou das relações entre variáveis ao longo do tempo, resultando em perda de performance do modelo.",
    "actual_output": "Olá!\n\nNo contexto fornecido, não há uma definição explícita do termo \"drift\". No entanto, podemos encontrar informações relevantes sobre o assunto.\n\nEm \"Model Evaluation and Deployment\", é mencionado que \"Drift errors occur when the live data is from a seemingly different distribution than the training set; these happen relatively slowly over time.\" Isso sugere que o drift se refere ao erro que ocorre quando os dados de treinamento e os dados em tempo real têm distribuições diferentes.\n\nEm resumo, no contexto fornecido, o drift se refere ao erro que ocorre quando os dados de treinamento e os dados em tempo real têm distribuições diferentes, levando a uma perda de desempenho do modelo.",
    "retrieval_context": [
      "developing machine learning models based on specific requirements. In\nthis phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing feature engineering to ensure the\ndata has the right format and structure.\nDuring model creation, various data pipelines are developed, enabling the\nsmooth flow of information between the different stages of the machine\nlearning process. Tools such as data engineering platforms can be used to\ndesign, test and maintain these pipelines.\nModel Training\nOnce the model has been created, it is trained using a suitable dataset.\nModel training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually",
      "driven by experimental code that is interactively written and executed in notebooks by data scientists, until a workable model is produced.\nDisconnection between ML and operations: The process separates data scientists who create the model and engineers who serve the model as a prediction service. The data scientists hand over a trained model as an artifact to the engineering team to deploy on their API infrastructure. This handoff can include putting the trained model\nin a storage location, checking the model object into a code repository, or uploading it to a models registry. Then engineers who deploy the model need to make the required features available in production for low-latency serving, which can lead to training-",
      "1 INTRODUCTION As Machine Learning (ML) models are increasingly incorporated into software, a nascent sub-field called MLOps (short for ML Op- erations) has emerged to organize the “set of practices that aim to deploy and maintain ML models in production reliably and effi- ciently” . It is widely agreed that MLOps is hard. Anecdotal re- ports claim that 90% of ML models don’t make it to production ; others claim that 85% of ML projects fail to deliver value .",
      ". Feature Engineering and Model Experimentation. ML engi- neers typically focus on improving ML performance, measured via metrics such as accuracy or mean-squared-error. Experiments can be data-driven or model-driven; for example, an engineer can create a new feature or change the model architecture from tree-based to neural network-based. Model Evaluation and Deployment. A model is typically eval- uated by computing a metric (e.g., accuracy) over a collection of labeled data points hidden at training time, or a validation dataset, to see if its performance is better than what the currently-running production model achieved during its evaluation phase",
      ". Drift errors occur when the live data is from a seemingly different distribution than the training set; these happen relatively slowly over time. One pain point mentioned by the in- terviewees was that different types of data errors require different responses, and it was not easy to determine the appropriate re- sponse. Another issue was that requiring practitioners to manually define constraints on data quality (e.g., lower and upper bounds on values) was not sustainable over time, as employees with this knowledge left the organization."
    ]
  },
  {
    "input": "O que é feature store?",
    "expected_output": "Uma feature store é um sistema que centraliza criação, versionamento, armazenamento e disponibilização de features para treinamento e inferência.",
    "actual_output": "Olá!\n\nDe acordo com o contexto, um feature store é uma república centralizada onde você standardiza a definição, armazenamento e acesso de recursos (features) para treinamento e serviço. Isso significa que o feature store fornece uma API para ambos o serviço em batch alto-throughput e o serviço em tempo real baixo-latência para os valores das características, e apoia tanto as cargas de treinamento quanto as cargas de serviço.\n\nO feature store ajuda cientistas de dados a descobrir e reutilizar conjuntos de recursos disponíveis para suas entidades, em vez de recriar os mesmos ou similares. Além disso, ele ajuda a manter recursos e seus metadados relacionados, evitando ter recursos semelhantes com definições diferentes.\n\nEspero que isso tenha ajudado!",
    "retrieval_context": [
      "access of features for training and serving. A feature store needs to provide an API for both high-throughput batch serving and low-latency real-time serving for the feature values, and to support both training and serving workloads.\nThe feature store helps data scientists do the following:\nDiscover and reuse available feature sets for their entities, instead of re-creating the same or similar ones.\nAvoid having similar features that have different definitions by maintaining features and their related metadata.\nServe up-to-date feature values from the feature store.\nAvoid training-serving skew by using the feature store as the data source for experimentation, continuous training, and online serving. This approach makes sure",
      "validation—in a canary deployment or an A/B testing setup—before it serves prediction for the online traffic.\nFeature store\nAn optional additional component for level 1 ML pipeline automation is a feature store. A feature store is a centralized repository where you standardize the definition, storage, and\naccess of features for training and serving. A feature store needs to provide an API for both high-throughput batch serving and low-latency real-time serving for the feature values, and to support both training and serving workloads.\nThe feature store helps data scientists do the following:\nDiscover and reuse available feature sets for their entities, instead of re-creating the same or similar ones.\nAvoid having similar features that have different definitions by maintaining features and their related metadata.\nServe up-to-date feature values from the feature store.",
      "that the features used for training are the same ones used during serving:\nFor experimentation, data scientists can get an offline extract from the feature store to run their experiments.\nFor continuous training, the automated ML training pipeline can fetch a batch of the up-to-date feature values of the dataset that are used for the training task.\nFor online prediction, the prediction service can fetch in a batch of the feature\nvalues related to the requested entity, such as customer demographic features, product features, and current session aggregation features.\nFor online prediction and feature retrieval, the prediction service identifies the",
      "Enable shareability, discoverability, reusability, and versioning of data assets.\nAllow real-time ingestion and low-latency serving for event streaming and online prediction workloads.\nAllow high-throughput batch ingestion and serving for extract, transform, load (ETL) processes and model\ntraining, and for scoring workloads.\n\nEnable feature versioning for point-in-time queries.",
      "Enable shareability, discoverability, reusability, and versioning of data assets.\nAllow real-time ingestion and low-latency serving for event streaming and online prediction workloads.\nAllow high-throughput batch ingestion and serving for extract, transform, load (ETL) processes and model\ntraining, and for scoring workloads.\n\nEnable feature versioning for point-in-time queries."
    ]
  },
  {
    "input": "O que significa servir um modelo em produção?",
    "expected_output": "Servir um modelo significa disponibilizá-lo por meio de um endpoint, job ou pipeline para receber dados e retornar previsões de forma escalável e confiável.",
    "actual_output": "Olá!\n\nNo contexto fornecido, não há uma definição específica do que significa \"servir um modelo em produção\". No entanto, podemos relacionar essa frase ao processo de Model Deployment, que é descrito como o processo de integrar o modelo no ambiente de produção, tornando-o acessível aos usuários finais.\n\nPortanto, servir um modelo em produção provavelmente significa fazer com que o modelo treinado seja utilizado e aplicado em um ambiente real, onde ele pode ser usado para tomar decisões ou realizar tarefas específicas.",
    "retrieval_context": [
      "Model training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually\nadjusted, and its performance is evaluated against a validation dataset to\nfine-tune its accuracy and effectiveness.\nSeveral techniques can be applied during the model training phase,\nincluding hyperparameter optimisation, cross-validation, and\nregularisation. Utilising the right combination of these methods helps\nreduce the risk of model overfitting and improve its generalisation\ncapabilities.\nModel Deployment\nAfter the model has been trained and its performance validated, it is time\nfor model deployment. Model deployment is the process of integrating\nthe model into the production environment, making it accessible to end-",
      "I’m gonna start with a model because it means faster . And often, like most of the time em- pirically, it’s gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\nPrior work has also identified the importance of data work .",
      "Store model metadata and runtime dependencies for deployability.\nMaintain model documentation and reporting—for example, using model cards.",
      "I’m gonna start with a model because it means faster . And often, like most of the time em- pirically, it’s gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\nPrior work has also identified the importance of data work .",
      "Store model metadata and runtime dependencies for deployability.\nMaintain model documentation and reporting—for example, using model cards."
    ]
  },
  {
    "input": "Por que versionar modelos é importante?",
    "expected_output": "Versionar modelos permite rastrear artefatos, reproduzir experimentos, auditar previsões e garantir rollback seguro.",
    "actual_output": "Olá!\n\nAqui está a resposta baseada no contexto:\n\nVersionar modelos é importante porque ajuda os engenheiros a debugar modelos, pois eles podem acessar as versões históricas dos recursos utilizados durante o treinamento de modelos (P3, P6, P14, P17). Isso é um exemplo da virtude de versioning.\n\nAlém disso, como mencionado no contexto, os engenheiros preferem ferramentas que melhoram significativamente sua experiência em relação às Three Vs (Section 4.2), o que inclui a velocidade de iterar sobre ideias de recursos ou modelos (P14, P15). Isso também é um motivador para versionar modelos.\n\nPortanto, versionar modelos é importante porque ajuda a melhorar a experiência dos engenheiros e facilita o processo de debugagem.",
    "retrieval_context": [
      "I’m gonna start with a model because it means faster . And often, like most of the time em- pirically, it’s gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\nPrior work has also identified the importance of data work .",
      "Additionally,wenoticedahigh-levelpatterninhowinterviewees discussed the tools they used: engineers seemed to prefer tools that significantly improved their experience with respect to the Three Vs (Section 4.2). For example, experiment tracking tools increased engineers’ speed of iterating on feature or modeling ideas (P14, P15)—avelocityvirtue.Inanotherexample,featurestores(i.e.,tables of derived columns for ML models) helped engineers debug models becausetheycouldaccesstherelevanthistoricalversionsoffeatures used in training such models (P3, P6, P14, P17)—a versioning virtue. MLOps tool builders may want to prioritize “10x” better experiences across velocity, validating early, or versioning for their products.",
      "Store model metadata and runtime dependencies for deployability.\nMaintain model documentation and reporting—for example, using model cards.",
      "I’m gonna start with a model because it means faster . And often, like most of the time em- pirically, it’s gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\nPrior work has also identified the importance of data work .",
      "Store model metadata and runtime dependencies for deployability.\nMaintain model documentation and reporting—for example, using model cards."
    ]
  },
  {
    "input": "O que é um pipeline de ML?",
    "expected_output": "É um fluxo automatizado que engloba etapas como ingestão, processamento, feature engineering, treinamento, avaliação e deploy.",
    "actual_output": "Olá!\n\nDe acordo com o contexto, um pipeline de ML (Machine Learning) é uma sequência de etapas que envolvem desenvolvimento e experimentação, integração contínua e entrega contínua. Essas etapas incluem:\n\n1. Desenvolvimento e experimentação: onde você testa novos algoritmos de aprendizado automático e modelagem;\n2. Pipeline continuous integration: onde o código fonte é construído e executado vários testes;\n3. Pipeline continuous delivery: onde os artefatos produzidos pela etapa anterior são implantados no ambiente de destino.\n\nEssa abordagem permite que engenheiros de ML tenham controle sobre os pipelines de ML, deployando-os usando processos e ferramentas de integração contínua e entrega contínua padrão.",
    "retrieval_context": [
      "The pipeline consists of the following stages:\n1. Development and experimentation: You iteratively try out new ML algorithms and new\nmodeling where the experiment steps are orchestrated. The output of this stage is the source code of the ML pipeline steps that are then pushed to a source repository.\n2. Pipeline continuous integration: You build source code and run various tests. The\noutputs of this stage are pipeline components (packages, executables, and artifacts) to be deployed in a later stage.\n3. Pipeline continuous delivery: You deploy the artifacts produced by the CI stage to the\ntarget environment. The output of this stage is a deployed pipeline with the new implementation of the model.",
      "control over the ML pipelines, ML engineers can deploy the pipeline using\nstandard CI/CD processes and tools. This approach is what the diagram\ndepicts. The diagram shows a standard CI/CD workflow, which consists of\nthese stages:\n1.\nIn the CI stage, the source code is unit-tested, and the training pipe-\nline is built and integration-tested. Any artifacts that are\ncreated by the build are stored in an artifact repository.\n19\nTraining\nOperationalization\nTypical assets produced in this process include the following:",
      "Pipeline Jungles. As a special case of glue code, pipeline jungles often appear in data prepara- tion. These can evolve organically, as new signals are identiﬁed and new information sources added incrementally. Without care, the resulting system for preparing data in an ML-friendly format may become a jungle of scrapes, joins, and sampling steps, often with intermediate ﬁles output. Man- aging these pipelines, detecting errors and recovering from failures are all difﬁcult and costly . Testing such pipelines often requires expensive end-to-end integration tests. All of this adds to technical debt of a system and makes further innovation more costly.",
      ".ml/ci streamlines data management and proposes unit tests for overfit- ting , Garg et al. survey different MLOps tools , and some papers introduce tools to perform validation and monitoring in production ML pipelines . MLOps Challenges. Sculley et al. were early proponents that pro- duction ML systems raise special challenges and can be hard to",
      ". survey different MLOps tools , and some papers introduce tools to perform validation and monitoring in production ML pipelines . MLOps Challenges. Sculley et al. were early proponents that pro- duction ML systems raise special challenges and can be hard to"
    ]
  },
  {
    "input": "O que é reprodutibilidade em ML?",
    "expected_output": "Capacidade de reproduzir os mesmos resultados usando o mesmo código, dados e parâmetros.",
    "actual_output": "Olá!\n\nA reprodução em ML (reproducibility) refere-se ao processo de garantir que os resultados obtidos com um modelo de aprendizado automático sejam replicáveis e consistentes, mesmo quando o modelo é executado novamente ou modificado. Isso significa que a reprodutibilidade é fundamental para garantir a confiabilidade e a transparência dos resultados em ML.\n\nNo contexto apresentado, a reprodutibilidade é mencionada como uma das funcionalidades-chave do ML metadata and artifact tracking, que permite rastrear a origem e a evolução de artefatos de ML, como modelos treinados e experimentos realizados. Isso facilita a reprodução dos resultados e ajuda a depurar problemas complexos em pipelines de ML.\n\nPortanto, a reprodutibilidade em ML é crucial para garantir que os resultados sejam replicáveis e consistentes, o que é fundamental para a confiabilidade e transparência dos modelos de aprendizado automático.",
    "retrieval_context": [
      "service).\nCT is a new property, unique to ML systems, that's concerned with automatically retraining and serving the models.\nThe following section discusses the typical steps for training and evaluating an ML model\nto serve as a prediction service.\nData science steps for ML\nIn any ML project, after you define the business use case and establish the success criteria, the process of delivering an ML model to production involves the following steps. These\nsteps can be completed manually or can be completed by an automatic pipeline.\n1. Data extraction: You select and integrate the relevant data from various data sources\nfor the ML task.\n2. Data analysis: You perform exploratory data analysis (EDA)",
      "A lot of ML is is like: people will claim to have like principled stances on why they did something and why it works. I think you can have intuitions that are use- ful and reasonable for why things should be good, but the most defining characteristic of is that he has the highest pace of experi- mentation out of anyone. He’s always running exper- iments, always trying everything. I think this is rel- atively common—people just try everything and then backfitsomenice-soundingexplanationforwhyitworks.",
      "statistics and data schemas, trained models, and evaluation results. ML metadata is the information about these\nartifacts, including their location, types, properties, and associations to experiments and runs. The ML metadata and\nartifact tracking capability is foundational to all other MLOps capabilities. Such a capability enables reproducibility\nand debugging of complex ML tasks and pipelines. Key functionalities in ML metadata and artifact tracking include\nthe following:\nProvide traceability and lineage tracking of ML artifacts.",
      "What is the task?\nHow can we measure business impact?\nWhat is the evaluation metric?\nFigure 5. The ML development process\n16\nWhat is the relevant data?\nWhat are the training and serving requirements?\nExperimentation aims to arrive at an effective prototype model for the ML use case at hand. In addition to experimen-\ntation, data scientists need to formalize their ML training procedures. They do this by implementing an end-to-end\npipeline, so that the procedures can be operationalized and run in production. Figure 5 shows the process of ML\ndevelopment.\nDuring experimentation, data scientists typically perform the following steps:\nData discovery, selection, and exploration.\nData preparation and feature engineering, using interactive data processing tools.\nModel prototyping and validation.",
      "like Terraform. Each environment is configured with its own set of required compute resources, data access, and\nsubset of MLOps capability services.\nML development\nExperimentation is the core activity in ML development, where your data scientists can rapidly try several ideas for\ndata preparation and ML modeling. Experimentation starts when the ML use case is well defined, meaning that the\nfollowing questions have been answered:\nWhat is the task?\nHow can we measure business impact?\nWhat is the evaluation metric?\nFigure 5. The ML development process\n16\nWhat is the relevant data?\nWhat are the training and serving requirements?\nExperimentation aims to arrive at an effective prototype model for the ML use case at hand. In addition to experimen-\ntation, data scientists need to formalize their ML training procedures. They do this by implementing an end-to-end\npipeline, so that the procedures can be operationalized and run in production. Figure 5 shows the process of ML\ndevelopment."
    ]
  },
  {
    "input": "O que é CI/CD aplicado a ML?",
    "expected_output": "CI/CD em ML automatiza testes, validação, empacotamento e implantação de modelos, garantindo ciclos mais rápidos e confiáveis.",
    "actual_output": "Olá!\n\nAqui está a resposta para sua pergunta: \n\nCI/CD (Continuous Integration e Continuous Delivery) em Machine Learning (ML) é um processo contínuo de integração, testagem e entrega de modelos de aprendizado automático. Isso significa que, ao invés de apenas testar e validar o código e os componentes, como é comum em sistemas de software tradicionais, o CI/CD em ML também envolve a testagem e validação de dados, esquemas de dados e modelos.\n\nAlém disso, o CD em ML não se refere apenas à entrega de um pacote de software ou serviço, mas sim ao deployment automático de um pipeline de treinamento de modelo que pode ser usado para servir outro serviço (como uma previsão de modelo).\n\nEspero que isso ajude!",
    "retrieval_context": [
      "monitor the online performance of your model to send notifications or roll back when values deviate from your expectations.\nML and other software systems are similar in continuous integration of source control, unit\ntesting, integration testing, and continuous delivery of the software module or the package. However, in ML, there are a few notable differences:\nCI is no longer only about testing and validating code and components, but also\ntesting and validating data, data schemas, and models.\nCD is no longer about a single software package or a service, but a system (an ML training pipeline) that should automatically deploy another service (model prediction\nservice).\nCT is a new property, unique to ML systems, that's concerned with automatically retraining and serving the models.",
      "testing, integration testing, and continuous delivery of the software module or the package. However, in ML, there are a few notable differences:\nCI is no longer only about testing and validating code and components, but also\ntesting and validating data, data schemas, and models.\nCD is no longer about a single software package or a service, but a system (an ML training pipeline) that should automatically deploy another service (model prediction\nservice).\nCT is a new property, unique to ML systems, that's concerned with automatically retraining and serving the models.\nThe following section discusses the typical steps for training and evaluating an ML model\nto serve as a prediction service.\nData science steps for ML\nIn any ML project, after you define the business use case and establish the success criteria, the process of delivering an ML model to production involves the following steps. These\nsteps can be completed manually or can be completed by an automatic pipeline.",
      "service).\nCT is a new property, unique to ML systems, that's concerned with automatically retraining and serving the models.\nThe following section discusses the typical steps for training and evaluating an ML model\nto serve as a prediction service.\nData science steps for ML\nIn any ML project, after you define the business use case and establish the success criteria, the process of delivering an ML model to production involves the following steps. These\nsteps can be completed manually or can be completed by an automatic pipeline.\n1. Data extraction: You select and integrate the relevant data from various data sources\nfor the ML task.\n2. Data analysis: You perform exploratory data analysis (EDA)",
      "4.4.4 ML evaluation metrics should be tied to product metrics. Mul- tiple participants stressed the importance of evaluating metrics critical to the product, such as click-through rate or user churn rate, rather than ML-specific metrics alone like MAP (P5, P7, P15, P16, P11, P17, P18, P19). The need to evaluate product-critical metrics stemmed from close collaboration with other stakeholders, such as product managers and business operators. P11 felt that a key reason many ML projects fail is that they don’t measure metrics that will yield the organization value:",
      "4.4.4 ML evaluation metrics should be tied to product metrics. Mul- tiple participants stressed the importance of evaluating metrics critical to the product, such as click-through rate or user churn rate, rather than ML-specific metrics alone like MAP (P5, P7, P15, P16, P11, P17, P18, P19). The need to evaluate product-critical metrics stemmed from close collaboration with other stakeholders, such as product managers and business operators. P11 felt that a key reason many ML projects fail is that they don’t measure metrics that will yield the organization value:"
    ]
  },
  {
    "input": "Por que monitorar um modelo em produção?",
    "expected_output": "Para detectar quedas de performance, drifts, falhas de pipeline, mudanças de distribuição e problemas de latência ou disponibilidade.",
    "actual_output": "Olá!\n\nMonitorar um modelo em produção é importante porque permite detectar mudanças no comportamento do modelo ao longo do tempo (model drift). Isso significa que, sem monitoramento, os sistemas de produção estariam \"voando cego\", ou seja, não teriam informações sobre como o modelo está se comportando.\n\nCom o monitoramento, a equipe de ciência de dados pode trabalhar de forma proativa em vez de reativa. Isso permite identificar problemas e atualizar o modelo antes que eles afetem negativamente a qualidade das previsões ou a continuidade do negócio.\n\nAlém disso, o monitoramento ajuda a garantir a precisão e confiabilidade dos modelos, validando tanto as previsões quanto os conjuntos de dados utilizados. Isso é fundamental para greenlighting (aprovando) modelos para produção.\n\nEspero que isso tenha ajudado!",
    "retrieval_context": [
      "Continuous Integration.\nMonitoring identifies model drift over time. Without model monitoring,\nproduction systems are flying blind. By monitoring for model drift the data\nscience team is able to proactively work rather than reactively.\nTesting ensures the accuracy and reliability of models. Validating both\nthe model’s predictions and the data sets used is a fundamental step in\ngreenlighting models for production.\nUse A/B testing to identify best models. A/B testing is sometimes\noverlooked in Machine Learning but is a great way to introduce new\nmodels. Rather than swapping models out straight away you can introduce\nthe new model alongside the old. This weighted approach allows you to\nsee the efficacy of the new model in production before committing to it.\n4. Version Control",
      "target environment. The output of this stage is a deployed pipeline with the new implementation of the model.\n4. Automated triggering: The pipeline is automatically executed in production based on a schedule or in response to a trigger. The output of this stage is a trained model that is pushed to the model registry.\n5. Model continuous delivery: You serve the trained model as a prediction service for the\npredictions. The output of this stage is a deployed model prediction service.\n6. Monitoring: You collect statistics on the model performance based on live data. The\noutput of this stage is a trigger to execute the pipeline or to execute a new experiment cycle.",
      "Support canary and shadow deployments.\n\nSupport traffic splitting and A/B tests.\n\nSupport multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production\nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to\nbe investigated and updated. Key functionalities in model monitoring include the following:\nMeasure model efficiency metrics like latency and serving-resource utilization.\nDetect data skews, including schema anomalies and data and concept shifts and drifts.",
      "Information about training, validation, and testing data splits that were used.\nModel evaluation metrics and the validation procedure that was used.\nIf there is no need to retrain the model on a regular basis, then the produced model at the end of the experimenta-\ntion is submitted to the model registry. The model is then ready to be reviewed, approved, and deployed to the target\n17\nserving environment. In addition, all the relevant metadata and artifacts\nthat were produced during model development are tracked in the metadata\ntracking repository.\nHowever, in most cases, ML models need to be retrained on a regular basis\nwhen new data is available or when the code changes. In this case, the\noutput of the ML development process is not the model to be deployed in",
      "Support canary and shadow deployments.\n\nSupport traffic splitting and A/B tests.\n\nSupport multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production\nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to\nbe investigated and updated. Key functionalities in model monitoring include the following:\nMeasure model efficiency metrics like latency and serving-resource utilization.\nDetect data skews, including schema anomalies and data and concept shifts and drifts."
    ]
  },
  {
    "input": "O que é um model registry?",
    "expected_output": "Repositório central para armazenar versões de modelos, métricas e histórico.",
    "actual_output": "Olá!\n\nDe acordo com o contexto, um Model Registry é uma capacidade que permite governar a vida ciclo de modelos de aprendizado de máquina (ML) em um repositório central. Isso garante a qualidade dos modelos de produção e habilita a descoberta de modelos.\n\nNo contexto, o Model Registry é mencionado como uma capacidade que permite registrar, organizar, rastrear e versionar seus modelos treinados e implantados. Além disso, também permite disparar pipelines em demanda, agendamento ou em resposta a eventos específicos.\n\nPortanto, um Model Registry é uma ferramenta que ajuda a gerenciar a vida ciclo de modelos de aprendizado de máquina, garantindo sua qualidade e facilitando a descoberta de novos modelos.",
    "retrieval_context": [
      "Store model metadata and runtime dependencies for deployability.\nMaintain model documentation and reporting—for example, using model cards.",
      "Store model metadata and runtime dependencies for deployability.\nMaintain model documentation and reporting—for example, using model cards.",
      "Enable local interactive execution for debugging during ML development.\n\nIntegrate with the ML metadata tracking capability to capture pipeline execution parameters and to produce\nartifacts.\nProvide a set of built-in components for common ML tasks and also allow custom components.\nRun on different environments, including local machines and scalable cloud platforms.\nOptionally, provide GUI-based tools for designing and building pipelines.\nModel registry\nThe model registry capability lets you govern the lifecycle of the ML models in a central repository. This ensures the\nquality of the production models and enables model discovery. Key functionalities in the model registry include the\nfollowing:\nRegister, organize, track, and version your trained and deployed ML models.",
      "Trigger pipelines on demand, on a schedule, or in response to specified events.\n\nEnable local interactive execution for debugging during ML development.\n\nIntegrate with the ML metadata tracking capability to capture pipeline execution parameters and to produce\nartifacts.\nProvide a set of built-in components for common ML tasks and also allow custom components.\nRun on different environments, including local machines and scalable cloud platforms.\nOptionally, provide GUI-based tools for designing and building pipelines.\nModel registry\nThe model registry capability lets you govern the lifecycle of the ML models in a central repository. This ensures the\nquality of the production models and enables model discovery. Key functionalities in the model registry include the\nfollowing:\nRegister, organize, track, and version your trained and deployed ML models.",
      "and model registry. Finally, two cross-cutting capabilities that enable integration and interaction are an ML metadata\nand artifact repository and an ML dataset and feature repository.\n10\nThe following sections outline the characteristics of each of the MLOps capabilities.\nExperimentation\nThe experimentation capability lets your data scientists and ML researchers collaboratively perform exploratory data\nanalysis, create prototype model architectures, and implement training routines. An ML environment should also let\nthem write modular, reusable, and testable source code that is version controlled. Key functionalities in experimenta-\ntion include the following:\nProvide notebook environments that are integrated with version control tools like Git."
    ]
  },
  {
    "input": "O que é data validation em MLOps?",
    "expected_output": "É o processo automatizado de verificar integridade, schema, ranges e consistência de dados antes de alimentar o pipeline de ML.",
    "actual_output": "Olá!\n\nDe acordo com o contexto, a data validation em MLOps refere-se ao processo de validação automática dos dados antes do treinamento do modelo para garantir que os resultados sejam esperados. Isso é importante porque, quando você deploy um modelo de aprendizado de máquina (ML) para produção, é necessário que o pipeline ML execute automaticamente e produza uma nova versão do modelo treinado com dados novos.\n\nPortanto, a data validation é um passo crítico no pipeline de produção para garantir que os resultados sejam esperados e evitar problemas de desempenho do modelo em produção.",
    "retrieval_context": [
      "MLOps: Continuous delivery and automation pipelines in machine learning | Cloud Architecture Center | Google Cloud Do…\nData and model validation\nWhen you deploy your ML pipeline to production, one or more of the triggers discussed in the ML pipeline triggers (#ml_pipeline_triggers) section automatically executes the pipeline. The pipeline expects new, live data to produce a new model version that is trained on the\nnew data (as shown in Figure 3). Therefore, automated data validation and model validation steps are required in the production pipeline to ensure the following expected behavior:\nData validation: This step is required before model training to decide whether you",
      ". We discuss work related to MLOps workflows, challenges and interview studies for ML. MLOps Workflow. The MLOps workflow involves supporting data collection and processing, experimentation, evaluation and de- ployment, and monitoring and response, as shown in Figure 1. Sev- eral research papers and companies have proposed tools to accom- plish various tasks in the workflow, such as data pre-processing and experiment tracking . Crankshaw et al. stud- ied the problem of model deployment and low-latency prediction serving . With regards to validating changes in production sys- tems,someresearchershavestudiedCI(ContinuousIntegration)for ML and proposed preliminary solutions—for example, ease.ml/ci streamlines data management and proposes unit tests for overfit- ting , Garg et al. survey different MLOps tools , and some papers introduce tools to perform validation and monitoring in production ML pipelines . MLOps Challenges. Sculley et al",
      "validated model as a prediction service that other applications can access through APIs.\nExploratory data analysis often requires you to experiment with diﬀerent models until the best model\nversion is ready for deployment. It leads to frequent model version deployments and data versioning.\nExperiment tracking and ML training pipeline management are essential before your applications can\nintegrate or consume the model in their code.\nMLOps is critical to systematically and simultaneously manage the release of new ML models with\napplication code and data changes. An optimal MLOps implementation treats the ML assets similarly to\nother continuous integration and delivery (CI/CD) environment software assets. You deploy ML models",
      ". survey different MLOps tools , and some papers introduce tools to perform validation and monitoring in production ML pipelines . MLOps Challenges. Sculley et al. were early proponents that pro- duction ML systems raise special challenges and can be hard to",
      "https://blog.paperspace.com/introduction-to-mlops/\n4/7\n11/11/25, 8:52 PM\nIntroduction to MLOps | Paperspace Blog\nMLOps: Best Practices\nCommunication and collaboration\nWhile the data scientist directs how the model should be built, you need a team of engineers and strategists to be successful. You should hire subject matter experts (also called SMEs), data scientists, software engineers, and business analysts to your team. Hence it is imperative that proper communication and collaboration are maintained.\nValidating the Dataset\nData Validation is one of the most important practices you should adopt. Once the model has been pushed to production, performance might degrade, and you might not get the correct predictions. This is why you should retrain the model even if it is a costly affair, both in terms of time and resources.\nSet up clear Business Objectives"
    ]
  },
  {
    "input": "Diferença entre batch inference e online inference?",
    "expected_output": "Batch inference processa grandes volumes de dados periodicamente, enquanto online inference responde em tempo real para requisições individuais.",
    "actual_output": "Olá!\n\nAqui está a resposta para sua pergunta:\n\nBatch inference refere-se ao processo de inferência em que um modelo é treinado com uma grande quantidade de dados (em lote) e, em seguida, utiliza esses mesmos dados para fazer previsões. Isso ocorre durante o treinamento do modelo.\n\nJá online inference se refere ao processo de inferência em que um modelo é usado para fazer previsões sobre novos dados que estão sendo gerados em tempo real. Nesse caso, o modelo não está mais trabalhando com os mesmos dados usados durante o treinamento, mas sim com novos dados que estão sendo adicionados ao modelo.\n\nPortanto, a principal diferença entre batch inference e online inference é que o primeiro ocorre durante o treinamento do modelo, enquanto o segundo ocorre após o treinamento, quando o modelo está sendo usado para fazer previsões sobre novos dados.\n\nEspero que isso tenha ajudado!",
    "retrieval_context": [
      "that the features used for training are the same ones used during serving:\nFor experimentation, data scientists can get an offline extract from the feature store to run their experiments.\nFor continuous training, the automated ML training pipeline can fetch a batch of the up-to-date feature values of the dataset that are used for the training task.\nFor online prediction, the prediction service can fetch in a batch of the feature\nvalues related to the requested entity, such as customer demographic features, product features, and current session aggregation features.\nFor online prediction and feature retrieval, the prediction service identifies the",
      "Support logging of prediction serving requests and responses for analysis.\nOnline experimentation\nThe online experimentation capability lets you understand how newly trained models perform in production settings\ncompared to the current models (if any) before you release the new model to production. For example, using a small\nsubset of the serving population, you use online experimentation to understand the impact that a new recommen-\ndation system has on click-throughs and on conversation rates. The results of online experimentation should be\nintegrated with the model registry capability to facilitate the decision about releasing the model to production. Online\nexperimentation enhances the reliability of your ML releases by helping you decide to discard ill-performing models",
      "experimentation enhances the reliability of your ML releases by helping you decide to discard ill-performing models\nand to promote well-performing ones. Key functionalities in online experimentation include the following:",
      "Support model explainability using techniques like feature attributions for a given model prediction.\n\nSupport logging of prediction serving requests and responses for analysis.\nOnline experimentation\nThe online experimentation capability lets you understand how newly trained models perform in production settings\ncompared to the current models (if any) before you release the new model to production. For example, using a small\nsubset of the serving population, you use online experimentation to understand the impact that a new recommen-\ndation system has on click-throughs and on conversation rates. The results of online experimentation should be\nintegrated with the model registry capability to facilitate the decision about releasing the model to production. Online\nexperimentation enhances the reliability of your ML releases by helping you decide to discard ill-performing models\nand to promote well-performing ones. Key functionalities in online experimentation include the following:",
      "Integrate with the model evaluation and deployment capability and track online and offline evaluation metrics\nfor the models.\nGovern the model launching process: review, approve, release, and roll back. These decisions are based on a\nnumber of offline performance and fairness metrics and on online experimentation results.\nDataset and feature repository\nThe dataset and feature repository capability lets you unify the definition and the storage of the ML data assets.\nHaving a central repository of fresh, high-quality data assets enables shareability, discoverability, and reusability. The\nrepository also provides data consistency for training and inference. This helps data scientists and ML researchers"
    ]
  },
  {
    "input": "O que é feature drift?",
    "expected_output": "É a mudança na distribuição estatística das features usadas pelo modelo ao longo do tempo.",
    "actual_output": "Olá!\n\nAqui no contexto, não há uma definição específica para \"feature drift\". No entanto, podemos relacionar essa pergunta à discussão sobre dados instáveis (unstable data dependencies) e mudanças naturais ou imprevistas nos dados.\n\nEm geral, o termo \"drift\" se refere a mudanças nos padrões de dados que podem afetar a performance de um modelo de aprendizado automático. Nesse sentido, podemos entender o \"feature drift\" como uma mudança nos padrões de valores de uma ou mais características (features) utilizadas para treinar e avaliar um modelo.\n\nNesse contexto, é importante notar que os participantes do estudo mencionaram exemplos de dados instáveis, como mudanças nas definições de palavras, formatos de dados corrompidos e dados recebidos de forma irregular. Essas mudanças podem ser consideradas um tipo de \"feature drift\", pois afetam a qualidade e a consistência dos dados utilizados para treinar e avaliar o modelo.\n\nPortanto, podemos concluir que o \"feature drift\" se refere à mudança nos padrões de valores de uma ou mais características (features) utilizadas para treinar e avaliar um modelo de aprendizado automático.",
    "retrieval_context": [
      "Consider the following examples. Feature A was incorrectly logged from 9/14 to 9/17. Feature B is not available on data before 10/7. The code used to compute feature C has to change for data before and after 11/1 because of changes to the logging format. Feature D is not available in production, so a substitute features D′ and D′′ must be used when querying the model in a live setting. If feature Z is used, then jobs for training must be given extra memory due to lookup tables or they will train inefﬁciently. Feature Q precludes the use of feature R because of latency constraints.",
      "Unstable Data Dependencies. To move quickly, it is often convenient to consume signals as input features that are produced by other systems. However, some input signals are unstable, meaning that they qualitatively or quantitatively change behavior over time. This can happen implicitly, when the input signal comes from another machine learning model itself that updates over time, or a data-dependent lookup table, such as for computing TF/IDF scores or semantic mappings. It can also happen explicitly, when the engineering ownership of the input signal is separate from the engineering ownership of the model that consumes it. In such cases, updates to the input signal may be made at any time",
      ". Several participants cited Covid as an example, but there are other (better) everyday instances of unnatural data drift. P6 de- scribed a bug where users had inconsistent definitions of the same word, complicating the deployment of a service to a new user. P7 mentioned a bug where data from users in a certain geographic region arrived more sporadically than usual. P10 discussed a bug where the format of raw data was occasionally corrupted: “Tables didn’t always have headers in the same place, even though they were the same tables.” • Natural data drift: Surprisingly, participants didn’t seem too worried about slower, expected natural data drift over time—they noted that frequent model retrains solved this problem (P6, P7, P8, P12, P15, P16, P17)",
      ". As an anecdote, we asked P17 to give an example of a natural data drift problem their company faced, and they could not think of a good example. P14 also said they don’t have natural data drift problems:",
      ". Several participants cited Covid as an example, but there are other (better) everyday instances of unnatural data drift. P6 de- scribed a bug where users had inconsistent definitions of the same word, complicating the deployment of a service to a new user. P7 mentioned a bug where data from users in a certain geographic region arrived more sporadically than usual. P10 discussed a bug where the format of raw data was occasionally corrupted: “Tables didn’t always have headers in the same place, even though they were the same tables.” • Natural data drift: Surprisingly, participants didn’t seem too worried about slower, expected natural data drift over time—they noted that frequent model retrains solved this problem (P6, P7, P8, P12, P15, P16, P17). As an anecdote, we asked P17 to give an example of a natural data drift problem their company faced, and they could not think of a good example. P14 also said they don’t have natural data drift problems:"
    ]
  },
  {
    "input": "O que é concept drift?",
    "expected_output": "É quando a relação entre as features e a variável alvo muda, tornando o modelo menos eficaz.",
    "actual_output": "Olá!\n\nNo contexto fornecido, não há uma definição explícita do conceito de \"concept drift\". No entanto, podemos identificar dois tipos de erros relacionados a mudanças nos dados: o \"drift errors\" e o \"natural data drift\".\n\nO \"drift errors\" ocorre quando os dados vivos são da mesma distribuição que o conjunto de treinamento, mas com uma mudança gradual ao longo do tempo. Isso pode ser causado por erros de qualidade nos dados ou pela falta de definições claras para a qualidade dos dados.\n\nJá o \"natural data drift\" se refere a mudanças esperadas e lentas nos dados ao longo do tempo, que podem ser resolvidas com re-treinamentos frequentes dos modelos. Os participantes não parecem preocupados com esse tipo de problema, pois eles são facilmente resolvidos com re-treinamentos.\n\nPortanto, podemos concluir que o \"concept drift\" provavelmente se refere à mudança gradual e esperada nos dados ao longo do tempo, que pode ser causada por fatores naturais ou erros de qualidade.",
    "retrieval_context": [
      "Undeclared Consumers. Oftentimes, a prediction from a machine learning model ma is made widely accessible, either at runtime or by writing to ﬁles or logs that may later be consumed by other systems. Without access controls, some of these consumers may be undeclared, silently using the output of a given model as an input to another system. In more classical software engineering, these issues are referred to as visibility debt .",
      ". Drift errors occur when the live data is from a seemingly different distribution than the training set; these happen relatively slowly over time. One pain point mentioned by the in- terviewees was that different types of data errors require different responses, and it was not easy to determine the appropriate re- sponse. Another issue was that requiring practitioners to manually define constraints on data quality (e.g., lower and upper bounds on values) was not sustainable over time, as employees with this knowledge left the organization.",
      ". Several participants cited Covid as an example, but there are other (better) everyday instances of unnatural data drift. P6 de- scribed a bug where users had inconsistent definitions of the same word, complicating the deployment of a service to a new user. P7 mentioned a bug where data from users in a certain geographic region arrived more sporadically than usual. P10 discussed a bug where the format of raw data was occasionally corrupted: “Tables didn’t always have headers in the same place, even though they were the same tables.” • Natural data drift: Surprisingly, participants didn’t seem too worried about slower, expected natural data drift over time—they noted that frequent model retrains solved this problem (P6, P7, P8, P12, P15, P16, P17)",
      ". As an anecdote, we asked P17 to give an example of a natural data drift problem their company faced, and they could not think of a good example. P14 also said they don’t have natural data drift problems:",
      ". Several participants cited Covid as an example, but there are other (better) everyday instances of unnatural data drift. P6 de- scribed a bug where users had inconsistent definitions of the same word, complicating the deployment of a service to a new user. P7 mentioned a bug where data from users in a certain geographic region arrived more sporadically than usual. P10 discussed a bug where the format of raw data was occasionally corrupted: “Tables didn’t always have headers in the same place, even though they were the same tables.” • Natural data drift: Surprisingly, participants didn’t seem too worried about slower, expected natural data drift over time—they noted that frequent model retrains solved this problem (P6, P7, P8, P12, P15, P16, P17). As an anecdote, we asked P17 to give an example of a natural data drift problem their company faced, and they could not think of a good example. P14 also said they don’t have natural data drift problems:"
    ]
  },
  {
    "input": "Para que serve o MLflow?",
    "expected_output": "Gerenciar experimentos, registrar métricas e armazenar modelos.",
    "actual_output": "Olá!\n\nA pergunta é sobre o MLflow, mas não há menção explícita ao MLflow no contexto fornecido. Portanto, não posso fornecer uma resposta específica sobre o que serve o MLflow.\n\nNenhuma informação disponível no contexto.",
    "retrieval_context": [
      ". We discuss work related to MLOps workflows, challenges and interview studies for ML. MLOps Workflow. The MLOps workflow involves supporting data collection and processing, experimentation, evaluation and de- ployment, and monitoring and response, as shown in Figure 1. Sev- eral research papers and companies have proposed tools to accom- plish various tasks in the workflow, such as data pre-processing and experiment tracking . Crankshaw et al. stud- ied the problem of model deployment and low-latency prediction serving . With regards to validating changes in production sys- tems,someresearchershavestudiedCI(ContinuousIntegration)for ML and proposed preliminary solutions—for example, ease.ml/ci streamlines data management and proposes unit tests for overfit- ting , Garg et al",
      "high-level flow of control and on key inputs and outputs.\nFigure 3. The MLOps process\nThis is not a waterfall workflow that has to sequentially pass through all the processes. The processes can be\nskipped, or the flow can repeat a given phase or a subsequence of the processes. The diagram shows the following\nflow:\n1. The core activity during this ML development phase is experimentation. As data scientists and ML research-\ners prototype model architectures and training routines, they create labeled datasets, and they use features\nand other reusable ML artifacts that are governed through the data and model management process. The\n8\nprimary output of this process is a formalized training procedure, which includes data preprocessing, model\narchitecture, and model training settings.\n2.",
      "Integrate monitoring with the model evaluation capability for continuously assessing the effectiveness\nperformance of the deployed model when ground truth labels are available.\nML pipelines\nThe ML pipelines capability lets you instrument, orchestrate, and automate complex ML training and prediction pipe-\n13\nlines in production. ML workflows coordinate different components, where each component performs a specific task\nin the pipeline. Key functionalities in ML pipelines include the following:\n\nTrigger pipelines on demand, on a schedule, or in response to specified events.\n\nEnable local interactive execution for debugging during ML development.",
      ". We discuss work related to MLOps workflows, challenges and interview studies for ML. MLOps Workflow. The MLOps workflow involves supporting data collection and processing, experimentation, evaluation and de- ployment, and monitoring and response, as shown in Figure 1. Sev- eral research papers and companies have proposed tools to accom- plish various tasks in the workflow, such as data pre-processing and experiment tracking . Crankshaw et al. stud- ied the problem of model deployment and low-latency prediction serving . With regards to validating changes in production sys- tems,someresearchershavestudiedCI(ContinuousIntegration)for ML and proposed preliminary solutions—for example, ease.ml/ci streamlines data management and proposes unit tests for overfit- ting , Garg et al. survey different MLOps tools , and some papers introduce tools to perform validation and monitoring in production ML pipelines . MLOps Challenges. Sculley et al",
      "Integrate monitoring with the model evaluation capability for continuously assessing the effectiveness\nperformance of the deployed model when ground truth labels are available.\nML pipelines\nThe ML pipelines capability lets you instrument, orchestrate, and automate complex ML training and prediction pipe-\n13\nlines in production. ML workflows coordinate different components, where each component performs a specific task\nin the pipeline. Key functionalities in ML pipelines include the following:\n\nTrigger pipelines on demand, on a schedule, or in response to specified events.\n\nEnable local interactive execution for debugging during ML development."
    ]
  },
  {
    "input": "O que é canary deployment em ML?",
    "expected_output": "É uma estratégia onde uma nova versão do modelo é implantada para uma pequena parcela do tráfego para validar performance antes do rollout completo.",
    "actual_output": "Olá!\n\nA pergunta sobre o que é canary deployment em ML me parece interessante! Embora não tenha encontrado uma resposta específica sobre canary deployment no contexto fornecido, posso tentar ajudar com base nas informações disponíveis.\n\nNo contexto, vemos que a discussão está relacionada às práticas de avaliação da implantação de ML e ao processo de MLOps (Machine Learning Operations). A frase \"multi-stage pattern\" sugere que o processo de implantação pode ser dividido em várias etapas ou fases.\n\nEm geral, canary deployment é uma técnica de implantação de software que envolve a criação de um ambiente isolado para testar e avaliar um novo recurso ou versão antes de implantá-lo em produção. Isso ajuda a minimizar o risco de problemas e permite que os desenvolvedores monitorem o desempenho do novo recurso antes de expandir para todos os usuários.\n\nEmbora não tenha encontrado uma resposta específica sobre canary deployment no contexto, é provável que essa técnica seja aplicada ao processo de MLOps, especialmente quando se trata da implantação de modelos de aprendizado de máquina em produção. No entanto, como não há informações específicas sobre isso no contexto, não posso confirmar ou negar a existência de canary deployment em ML.\n\nSe você tiver mais informações ou contexto adicional, estou aqui para ajudar!",
    "retrieval_context": [
      "Takeaway. While most participants were unable to share exact information about the length of the staged deployment process and specific anecdotes about experiments they needed to cancel for privacy reasons, we found it interesting how different organiza- tions had different deployment evaluation practices yet similar pain around failed project ideas due to the highly iterative, experimen- tal nature of ML. We believe there is an opportunity for tooling to streamline ML deployments in this multi-stage pattern, to minimize wasted work and help practitioners predict the end-to-end gains for their ideas.\nShreya Shankar∗, Rolando Garcia∗, Joseph M. Hellerstein, Aditya G. Parameswaran",
      "ABSTRACT Organizations rely on machine learning engineers (MLEs) to opera- tionalize ML, i.e., deploy and maintain ML pipelines in production. The process of operationalizing ML, or MLOps, consists of a contin- ual loop of (i) data collection and labeling, (ii) experimentation to improveMLperformance,(iii)evaluationthroughoutamulti-staged deployment process, and (iv) monitoring of performance drops in production. When considered together, these responsibilities seem staggering—how does anyone do MLOps, what are the unaddressed challenges, and what are the implications for tool builders?",
      "set of core technical capabilities. These capabilities can be provided by a single integrated ML platform. Alternative-\nly, they can be created by combining vendor tools that each are best suited to particular tasks, developed as custom\nservices, or created as a combination of these approaches.\nIn most cases, the processes are deployed in stages rather than all at once in a single deployment. An organization’s\nplan for adopting these processes and capabilities should align with business priorities and with the organization’s\ntechnical and skills maturity. For example, many organizations start by focusing on the processes for ML develop-\nment, model deployment, and prediction serving. For these organizations, continuous training and continuous moni-",
      "resource utilization, and execution errors.\nMLOps capabilities\nTo effectively implement the key MLOps processes outlined in the previous section, organizations need to establish a\nset of core technical capabilities. These capabilities can be provided by a single integrated ML platform. Alternative-\nly, they can be created by combining vendor tools that each are best suited to particular tasks, developed as custom\nservices, or created as a combination of these approaches.\nIn most cases, the processes are deployed in stages rather than all at once in a single deployment. An organization’s\nplan for adopting these processes and capabilities should align with business priorities and with the organization’s\ntechnical and skills maturity. For example, many organizations start by focusing on the processes for ML develop-\nment, model deployment, and prediction serving. For these organizations, continuous training and continuous moni-",
      "Integrate with all other MLOps capabilities.\nDeep dive of MLOps processes\nThis section describes each of the core MLOps processes in detail. It describes key tasks and flow of control be-\ntween tasks, the key artifacts created by the tasks, and the relationship of tasks to other upstream and downstream\nprocesses. In this section, you learn about concrete details of tasks like running a continuous training pipeline, de-\nploying a model, and monitoring predictive performance of the model.\n15\nMLOps processes take place on an integrated ML platform that has the required development and operations capa-\nbilities (described later). Infrastructure engineers can provision this type of platform in different environments (like\ndevelopment, test, staging, and production) using configuration management and infrastructure-as-code (IaC) tools\nlike Terraform. Each environment is configured with its own set of required compute resources, data access, and\nsubset of MLOps capability services."
    ]
  },
  {
    "input": "O que é pipeline de retreinamento de modelo?",
    "expected_output": "Pipeline automático que treina novamente o modelo com novos dados ou drift.",
    "actual_output": "Olá!\n\nAqui, no contexto fornecido, não há uma definição específica para \"pipeline de retraining de modelo\". No entanto, podemos relacionar a discussão sobre pipelines e modelos com o conceito de retraining.\n\nO pipeline descrito consiste em três estágios: desenvolvimento e experimentação, integração contínua e entrega contínua. O objetivo é iterativamente testar novos algoritmos e modelos, produzir artefatos e deploy-los no ambiente de destino.\n\nA discussão sobre process management debt e a necessidade de ferramentas para ajudar na recuperação de incidentes em pipelines pode ser relacionada à necessidade de retraining de modelos. Isso ocorre porque os modelos precisam ser atualizados regularmente para manter-se eficazes e adaptados às mudanças nos dados, produtos e requisitos.\n\nPortanto, podemos inferir que o pipeline de retraining de modelo se refere ao processo de iterativamente desenvolver, testar e deploy-los novos algoritmos e modelos para atualizar os modelos existentes e manter a eficácia dos pipelines.",
    "retrieval_context": [
      "The pipeline consists of the following stages:\n1. Development and experimentation: You iteratively try out new ML algorithms and new\nmodeling where the experiment steps are orchestrated. The output of this stage is the source code of the ML pipeline steps that are then pushed to a source repository.\n2. Pipeline continuous integration: You build source code and run various tests. The\noutputs of this stage are pipeline components (packages, executables, and artifacts) to be deployed in a later stage.\n3. Pipeline continuous delivery: You deploy the artifacts produced by the CI stage to the\ntarget environment. The output of this stage is a deployed pipeline with the new implementation of the model.",
      "Process Management Debt. Most of the use cases described in this paper have talked about the cost of maintaining a single model, but mature systems may have dozens or hundreds of models running simultaneously . This raises a wide range of important problems, including the problem of updating many conﬁgurations for many similar models safely and automatically, how to manage and assign resources among models with different business priorities, and how to visualize and detect blockages in the ﬂow of data in a production pipeline. Developing tooling to aid recovery from production incidents is also critical. An important system-level smell to avoid are common processes with many manual steps.",
      "4.4 Operationalizing Model Evaluation is an\nActive Effort\nWe found that MLEs described intensive model evaluation efforts at their companies to keep up with data changes, product and business requirement changes, user changes, and organizational changes. The goal of model evaluation is to prevent repeated failures and bad models from making it to production while maintaining velocity— i.e., the ability for pipelines to quickly adapt to change.",
      "Process Management Debt. Most of the use cases described in this paper have talked about the cost of maintaining a single model, but mature systems may have dozens or hundreds of models running simultaneously . This raises a wide range of important problems, including the problem of updating many conﬁgurations for many similar models safely and automatically, how to manage and assign resources among models with different business priorities, and how to visualize and detect blockages in the ﬂow of data in a production pipeline. Developing tooling to aid recovery from production incidents is also critical. An important system-level smell to avoid are common processes with many manual steps.",
      "4.4 Operationalizing Model Evaluation is an\nActive Effort\nWe found that MLEs described intensive model evaluation efforts at their companies to keep up with data changes, product and business requirement changes, user changes, and organizational changes. The goal of model evaluation is to prevent repeated failures and bad models from making it to production while maintaining velocity— i.e., the ability for pipelines to quickly adapt to change."
    ]
  },
  {
    "input": "O que caracteriza um bom ambiente de MLOps?",
    "expected_output": "Um ambiente com versionamento consistente, pipelines automatizados, monitoramento contínuo, logging estruturado, validação de dados e processos reprodutíveis.",
    "actual_output": "Olá!\n\nAqui está a resposta para sua pergunta:\n\nUm bom ambiente de MLOps é caracterizado por ter as seguintes características: \n\n* Possui uma plataforma integrada de ML com capacidades de desenvolvimento e operações (MLOps) que incluem processos padronizados e tecnologias necessárias para construir, implantar e operacionalizar sistemas de ML rapidamente e confiavelmente.\n* Permite a provisionamento de ambientes diferentes (desenvolvimento, test, staging e produção) usando ferramentas de gerenciamento de configuração e infraestrutura como código (IaC), como o Terraform.\n* Cada ambiente é configurado com seus próprios recursos computacionais necessários, acesso a dados e subconjunto de serviços de MLOps.\n\nEssas características permitem que os times de desenvolvimento de ML trabalhem de forma eficaz e escalável, garantindo a entrega rápida e confiável de modelos de aprendizado automático para apoiar as necessidades da empresa.",
    "retrieval_context": [
      "operations (the Ops element). It advocates formalizing and (when beneficial) automating critical steps of ML system\nconstruction. MLOps provides a set of standardized processes and technology capabilities for building, deploying,\nand operationalizing ML systems rapidly and reliably.\nMLOps supports ML development and deployment in the way that DevOps and DataOps support application engi-\nneering and data engineering (analytics). The difference is that when you deploy a web service, you care about resil-\nience, queries per second, load balancing, and so on. When you deploy an ML model, you also need to worry about\nchanges in the data, changes in the model, users trying to game the system, and so on. This is what MLOps is about.",
      "such as business intelligence systems, line of business applications, process control systems, and embedded sys-\ntems. Integrating an ML model into an application is a critical task that involves making sure first that the deployed\nmodel is used effectively by the applications, and then monitoring model performance. In addition to this, you should also collect and monitor relevant business KPIs (for example, click-through rate, revenue uplift, and user experience).\nThis information helps you understand the impact of the ML model on the business and adapt accordingly.\n6\nThe MLOps lifecycle\nThe MLOps lifecycle encompasses seven integrated and iterative processes, as shown in figure 2.\nFigure 2. The MLOps lifecycle\nThe processes can consist of the following:",
      "high-level flow of control and on key inputs and outputs.\nFigure 3. The MLOps process\nThis is not a waterfall workflow that has to sequentially pass through all the processes. The processes can be\nskipped, or the flow can repeat a given phase or a subsequence of the processes. The diagram shows the following\nflow:\n1. The core activity during this ML development phase is experimentation. As data scientists and ML research-\ners prototype model architectures and training routines, they create labeled datasets, and they use features\nand other reusable ML artifacts that are governed through the data and model management process. The\n8\nprimary output of this process is a formalized training procedure, which includes data preprocessing, model\narchitecture, and model training settings.\n2.",
      "Integrate with all other MLOps capabilities.\nDeep dive of MLOps processes\nThis section describes each of the core MLOps processes in detail. It describes key tasks and flow of control be-\ntween tasks, the key artifacts created by the tasks, and the relationship of tasks to other upstream and downstream\nprocesses. In this section, you learn about concrete details of tasks like running a continuous training pipeline, de-\nploying a model, and monitoring predictive performance of the model.\n15\nMLOps processes take place on an integrated ML platform that has the required development and operations capa-\nbilities (described later). Infrastructure engineers can provision this type of platform in different environments (like\ndevelopment, test, staging, and production) using configuration management and infrastructure-as-code (IaC) tools\nlike Terraform. Each environment is configured with its own set of required compute resources, data access, and\nsubset of MLOps capability services.",
      "like Terraform. Each environment is configured with its own set of required compute resources, data access, and\nsubset of MLOps capability services.\nML development\nExperimentation is the core activity in ML development, where your data scientists can rapidly try several ideas for\ndata preparation and ML modeling. Experimentation starts when the ML use case is well defined, meaning that the\nfollowing questions have been answered:\nWhat is the task?\nHow can we measure business impact?\nWhat is the evaluation metric?\nFigure 5. The ML development process\n16\nWhat is the relevant data?\nWhat are the training and serving requirements?\nExperimentation aims to arrive at an effective prototype model for the ML use case at hand. In addition to experimen-\ntation, data scientists need to formalize their ML training procedures. They do this by implementing an end-to-end\npipeline, so that the procedures can be operationalized and run in production. Figure 5 shows the process of ML\ndevelopment."
    ]
  },
  {
    "input": "O que é um modelo de machine learning?",
    "expected_output": "Um modelo de machine learning é um algoritmo treinado com dados para fazer previsões ou classificações.",
    "actual_output": "Olá!\n\nDe acordo com o contexto, um modelo de machine learning é descrito como uma entidade que é treinada usando um conjunto de dados apropriado. O processo de treinamento do modelo envolve alimentar dados para ele aprender e fazer previsões. O modelo é ajustado continuamente e sua performance é avaliada em relação a um conjunto de dados de validação para fine-tuning sua precisão e efetividade.\n\nPortanto, podemos definir um modelo de machine learning como uma estrutura que aprende com os dados fornecidos e é capaz de fazer previsões ou tomadas de decisão baseadas nesses dados.",
    "retrieval_context": [
      "developing machine learning models based on specific requirements. In\nthis phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing feature engineering to ensure the\ndata has the right format and structure.\nDuring model creation, various data pipelines are developed, enabling the\nsmooth flow of information between the different stages of the machine\nlearning process. Tools such as data engineering platforms can be used to\ndesign, test and maintain these pipelines.\nModel Training\nOnce the model has been created, it is trained using a suitable dataset.\nModel training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually",
      "this phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing feature engineering to ensure the\ndata has the right format and structure.\nDuring model creation, various data pipelines are developed, enabling the\nsmooth flow of information between the different stages of the machine\nlearning process. Tools such as data engineering platforms can be used to\ndesign, test and maintain these pipelines.\nModel Training\nOnce the model has been created, it is trained using a suitable dataset.\nModel training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually\nadjusted, and its performance is evaluated against a validation dataset to\nfine-tune its accuracy and effectiveness.\nSeveral techniques can be applied during the model training phase,\nincluding hyperparameter optimisation, cross-validation, and\nregularisation. Utilising the right combination of these methods helps",
      "1 INTRODUCTION As Machine Learning (ML) models are increasingly incorporated into software, a nascent sub-field called MLOps (short for ML Op- erations) has emerged to organize the “set of practices that aim to deploy and maintain ML models in production reliably and effi- ciently” . It is widely agreed that MLOps is hard. Anecdotal re- ports claim that 90% of ML models don’t make it to production ; others claim that 85% of ML projects fail to deliver value .",
      ". Feature Engineering and Model Experimentation. ML engi- neers typically focus on improving ML performance, measured via metrics such as accuracy or mean-squared-error. Experiments can be data-driven or model-driven; for example, an engineer can create a new feature or change the model architecture from tree-based to neural network-based. Model Evaluation and Deployment. A model is typically eval- uated by computing a metric (e.g., accuracy) over a collection of labeled data points hidden at training time, or a validation dataset, to see if its performance is better than what the currently-running production model achieved during its evaluation phase",
      "high-level flow of control and on key inputs and outputs.\nFigure 3. The MLOps process\nThis is not a waterfall workflow that has to sequentially pass through all the processes. The processes can be\nskipped, or the flow can repeat a given phase or a subsequence of the processes. The diagram shows the following\nflow:\n1. The core activity during this ML development phase is experimentation. As data scientists and ML research-\ners prototype model architectures and training routines, they create labeled datasets, and they use features\nand other reusable ML artifacts that are governed through the data and model management process. The\n8\nprimary output of this process is a formalized training procedure, which includes data preprocessing, model\narchitecture, and model training settings.\n2.\nIf the ML system requires continuous training (repeated retraining of the model), the training procedure is"
    ]
  },
  {
    "input": "O que é treinamento de modelo?",
    "expected_output": "Treinamento é o processo de ajustar os parâmetros do modelo usando dados rotulados ou não rotulados.",
    "actual_output": "Olá!\n\nAqui no contexto, não há uma definição específica do que é treinamento de modelo. No entanto, podemos extrair algumas informações relevantes sobre modelos e seu relacionamento com dados.\n\nNo primeiro trecho, o autor menciona que \"it's gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\" Isso sugere que os modelos estão relacionados à análise de dados e ao uso desses dados para melhorar ou \"push the boundary\" (empurrar o limite).\n\nJá no segundo trecho, há referências a \"model metadata and runtime dependencies for deployability\" e \"Maintain model documentation and reporting—for example, using model cards.\" Isso sugere que os modelos têm metadados e dependências de tempo de execução que precisam ser armazenadas e documentadas para fins de deployabilidade.\n\nEmbora não haja uma definição explícita do treinamento de modelo, podemos inferir que o treinamento de modelo é um processo que envolve a análise de dados e a criação ou ajuste de modelos com base nesses dados.",
    "retrieval_context": [
      "I’m gonna start with a model because it means faster . And often, like most of the time em- pirically, it’s gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\nPrior work has also identified the importance of data work .",
      "Store model metadata and runtime dependencies for deployability.\nMaintain model documentation and reporting—for example, using model cards.",
      "Process Management Debt. Most of the use cases described in this paper have talked about the cost of maintaining a single model, but mature systems may have dozens or hundreds of models running simultaneously . This raises a wide range of important problems, including the problem of updating many conﬁgurations for many similar models safely and automatically, how to manage and assign resources among models with different business priorities, and how to visualize and detect blockages in the ﬂow of data in a production pipeline. Developing tooling to aid recovery from production incidents is also critical. An important system-level smell to avoid are common processes with many manual steps.",
      "I’m gonna start with a model because it means faster . And often, like most of the time em- pirically, it’s gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\nPrior work has also identified the importance of data work .",
      "Store model metadata and runtime dependencies for deployability.\nMaintain model documentation and reporting—for example, using model cards."
    ]
  },
  {
    "input": "Por que separar treino e teste?",
    "expected_output": "A separação evita overfitting e garante avaliação imparcial da performance.",
    "actual_output": "Amigo estudante!\n\nA pergunta é sobre a razão para separar o treinamento (treino) do modelo de seu teste. Isso é uma prática comum em Machine Learning, pois permite avaliar a performance do modelo de forma independente e evitar overfitting.\n\nNo contexto, não há informações específicas sobre por que separar treino e teste, mas podemos inferir que isso é feito para garantir que o modelo seja testado de forma objetiva e não seja influenciado pelas características do conjunto de dados de treinamento. Isso ajuda a evitar que o modelo se adapte excessivamente ao conjunto de dados de treinamento e não generalize bem para novos conjuntos de dados.\n\nEm resumo, separar treino e teste é uma boa prática em Machine Learning porque permite avaliar a performance do modelo de forma independente e evitar overfitting.",
    "retrieval_context": [
      "Pipeline Jungles. As a special case of glue code, pipeline jungles often appear in data prepara- tion. These can evolve organically, as new signals are identiﬁed and new information sources added incrementally. Without care, the resulting system for preparing data in an ML-friendly format may become a jungle of scrapes, joins, and sampling steps, often with intermediate ﬁles output. Man- aging these pipelines, detecting errors and recovering from failures are all difﬁcult and costly . Testing such pipelines often requires expensive end-to-end integration tests. All of this adds to technical debt of a system and makes further innovation more costly.",
      "Multiple-Language Smell. It is often tempting to write a particular piece of a system in a given language, especially when that language has a convenient library or syntax for the task at hand. However, using multiple languages often increases the cost of effective testing and can increase the difﬁculty of transferring ownership to other individuals.",
      "We discuss three examples of point points caused by the envi- ronment mismatch—data leakage, Jupyter notebook philosophies, and code quality.",
      "Experimentation\nThe experimentation capability lets your data scientists and ML researchers collaboratively perform exploratory data\nanalysis, create prototype model architectures, and implement training routines. An ML environment should also let\nthem write modular, reusable, and testable source code that is version controlled. Key functionalities in experimenta-\ntion include the following:\nProvide notebook environments that are integrated with version control tools like Git.",
      "Support canary and shadow deployments.\n\nSupport traffic splitting and A/B tests.\n\nSupport multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production\nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to\nbe investigated and updated. Key functionalities in model monitoring include the following:\nMeasure model efficiency metrics like latency and serving-resource utilization.\nDetect data skews, including schema anomalies and data and concept shifts and drifts."
    ]
  },
  {
    "input": "O que é inferência?",
    "expected_output": "Inferência é o uso de um modelo treinado para gerar previsões em novos dados.",
    "actual_output": "Olá!\n\nA pergunta é sobre o que é inferência?\n\nInfere-se que a resposta está relacionada ao contexto, pois não há informações específicas sobre inferência no texto. No entanto, podemos encontrar uma referência à inferência em relação às predições feitas por um modelo de aprendizado de máquina (machine learning model).\n\nEm resumo, a inferência é o processo de fazer predições ou prever resultados com base nos dados e modelos treinados. Nesse contexto, as predições podem ser feitas por um modelo de aprendizado de máquina e são frequentemente acessíveis para outros sistemas.\n\nNenhuma informação disponível no contexto.",
    "retrieval_context": [
      "Undeclared Consumers. Oftentimes, a prediction from a machine learning model ma is made widely accessible, either at runtime or by writing to ﬁles or logs that may later be consumed by other systems. Without access controls, some of these consumers may be undeclared, silently using the output of a given model as an input to another system. In more classical software engineering, these issues are referred to as visibility debt .",
      "2\nUndeclared consumers may be difﬁcult to detect unless the system is speciﬁcally designed to guard against this case, for example with access restrictions or strict service-level agreements (SLAs). In the absence of barriers, engineers will naturally use the most convenient signal at hand, especially when working against deadline pressures.\n3 Data Dependencies Cost More than Code Dependencies",
      "I’m gonna start with a model because it means faster . And often, like most of the time em- pirically, it’s gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\nPrior work has also identified the importance of data work .",
      "I’m gonna start with a model because it means faster . And often, like most of the time em- pirically, it’s gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\nPrior work has also identified the importance of data work .",
      "Experimentation\nThe experimentation capability lets your data scientists and ML researchers collaboratively perform exploratory data\nanalysis, create prototype model architectures, and implement training routines. An ML environment should also let\nthem write modular, reusable, and testable source code that is version controlled. Key functionalities in experimenta-\ntion include the following:\nProvide notebook environments that are integrated with version control tools like Git."
    ]
  },
  {
    "input": "O que é um dataset?",
    "expected_output": "Dataset é uma coleção estruturada de dados utilizada para treinamento, validação ou teste.",
    "actual_output": "Olá!\n\nNo contexto fornecido, um dataset é mencionado como uma forma de dados que pode ser utilizada para treinar modelos de aprendizado automático (ML). Um dataset é descrito como contendo características de entidades, como por exemplo, o conjunto de dados de clientes (customer churn dataset), que inclui características como idade, código postal e gênero.\n\nAlém disso, um dataset também pode ser mencionado como sendo gerenciado em níveis diferentes, como o nível de características da entidade ou o nível do conjunto de dados completo. Isso permite que os dados sejam organizados e trabalhados de forma mais eficiente.\n\nPortanto, podemos concluir que um dataset é uma coleção de dados estruturados e/ou não estruturados (como texto, imagens, etc.) que pode ser utilizada para treinar modelos de aprendizado automático.",
    "retrieval_context": [
      "Support interactive execution (for example, from notebooks) for quick experimentation and for long-running\njobs in production.\nProvide data connectors to a wide range of data sources and services, as well as data encoders and\ndecoders for various data structures and formats.\nProvide both rich and efficient data transformations and ML feature engineering for structured (tabular) and\nunstructured data (text, image, and so on).",
      "Support various data modalities, including tabular data, images, and text.\nML data assets can be managed at the entity features level or at the full dataset level. For example, a feature reposi-\ntory might contain an entity called customer, which includes features like age group, postal code, and gender. On the\nother hand, a dataset repository might include a customer churn dataset, which includes features from the customer\nand product entities, as well as purchase- and web-activity event logs.\nML metadata and artifact tracking\nVarious types of ML artifacts are produced in different processes of the MLOps lifecycle, including descriptive\nstatistics and data schemas, trained models, and evaluation results. ML metadata is the information about these",
      "Data preparation and feature engineering, using interactive data processing tools.\nModel prototyping and validation.\nPerforming these iterative steps can lead data scientists to refining the problem definition. For example, your data\nscientists or researchers might change the task from regression to classification, or they might opt for another evalu-\nation metric.\nThe primary source of development data is the dataset and feature repository. This repository contains curated data\nassets that are managed on either the entity-features level or the full dataset level.\nIn general, the key success aspects for this process are experiment tracking, reproducibility, and collaboration. For",
      "this phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing feature engineering to ensure the\ndata has the right format and structure.\nDuring model creation, various data pipelines are developed, enabling the\nsmooth flow of information between the different stages of the machine\nlearning process. Tools such as data engineering platforms can be used to\ndesign, test and maintain these pipelines.\nModel Training\nOnce the model has been created, it is trained using a suitable dataset.\nModel training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually\nadjusted, and its performance is evaluated against a validation dataset to\nfine-tune its accuracy and effectiveness.\nSeveral techniques can be applied during the model training phase,\nincluding hyperparameter optimisation, cross-validation, and\nregularisation. Utilising the right combination of these methods helps",
      "Support various data modalities, including tabular data, images, and text.\nML data assets can be managed at the entity features level or at the full dataset level. For example, a feature reposi-\ntory might contain an entity called customer, which includes features like age group, postal code, and gender. On the\nother hand, a dataset repository might include a customer churn dataset, which includes features from the customer\nand product entities, as well as purchase- and web-activity event logs.\nML metadata and artifact tracking\nVarious types of ML artifacts are produced in different processes of the MLOps lifecycle, including descriptive\nstatistics and data schemas, trained models, and evaluation results. ML metadata is the information about these\nartifacts, including their location, types, properties, and associations to experiments and runs. The ML metadata and\nartifact tracking capability is foundational to all other MLOps capabilities. Such a capability enables reproducibility"
    ]
  },
  {
    "input": "Por que usar pipelines em ML?",
    "expected_output": "Pipelines organizam e automatizam as etapas do ciclo de vida de ML, garantindo reprodutibilidade e escalabilidade.",
    "actual_output": "Olá!\n\nA pergunta é sobre por que usar pipelines em Machine Learning (ML). Vou buscar informações no contexto para te dar uma resposta clara e informativa.\n\nDe acordo com o contexto, os pipelines de ML são usados para instrumentar, orquestrar e automatizar complexos treinamentos e previsões de ML em produção. Além disso, eles permitem coordenar diferentes componentes, onde cada componente realiza uma tarefa específica no pipeline.\n\nA resposta à pergunta é que os pipelines em ML são usados para:\n\n* Instrumentar e orquestrar complexos treinamentos e previsões de ML em produção;\n* Coordenar diferentes componentes para realizar tarefas específicas;\n* Automatizar processos e reduzir a necessidade de intervenções humanas;\n* Fornecer uma visão geral do pipeline e facilitar a detecção de erros e recuperação de falhas.\n\nAlém disso, os pipelines em ML também ajudam a gerenciar o conhecimento tribal não documentado entre colaboradores com mais experiência relacionada a específicos pipelines. Isso ajuda a manter a documentação atualizada e a evitar a perda de conhecimento importante.\n\nEspero que isso tenha ajudado!",
    "retrieval_context": [
      "Integrate monitoring with the model evaluation capability for continuously assessing the effectiveness\nperformance of the deployed model when ground truth labels are available.\nML pipelines\nThe ML pipelines capability lets you instrument, orchestrate, and automate complex ML training and prediction pipe-\n13\nlines in production. ML workflows coordinate different components, where each component performs a specific task\nin the pipeline. Key functionalities in ML pipelines include the following:\n\nTrigger pipelines on demand, on a schedule, or in response to specified events.\n\nEnable local interactive execution for debugging during ML development.",
      "Integrate monitoring with the model evaluation capability for continuously assessing the effectiveness\nperformance of the deployed model when ground truth labels are available.\nML pipelines\nThe ML pipelines capability lets you instrument, orchestrate, and automate complex ML training and prediction pipe-\n13\nlines in production. ML workflows coordinate different components, where each component performs a specific task\nin the pipeline. Key functionalities in ML pipelines include the following:\n\nTrigger pipelines on demand, on a schedule, or in response to specified events.\n\nEnable local interactive execution for debugging during ML development.",
      ". survey different MLOps tools , and some papers introduce tools to perform validation and monitoring in production ML pipelines . MLOps Challenges. Sculley et al. were early proponents that pro- duction ML systems raise special challenges and can be hard to",
      "Pipeline Jungles. As a special case of glue code, pipeline jungles often appear in data prepara- tion. These can evolve organically, as new signals are identiﬁed and new information sources added incrementally. Without care, the resulting system for preparing data in an ML-friendly format may become a jungle of scrapes, joins, and sampling steps, often with intermediate ﬁles output. Man- aging these pipelines, detecting errors and recovering from failures are all difﬁcult and costly . Testing such pipelines often requires expensive end-to-end integration tests. All of this adds to technical debt of a system and makes further innovation more costly.",
      "satisfy customers and help organize teams around a roadmap of experiment ideas, maybe they are not so bad.\n5.2.4 Undocumented Tribal Knowledge. P6, P10, P13, P14, P16, P17, and P19 each discussed pain points related to undocumented knowledge about ML experiments and pipelines amongst collab- orators with more experience related to specific pipelines. Across interviews, it seemed like high velocity created many versions, which made it hard to maintain up-to-date documentation. P10 mentioned that there were parts of a pipeline that no one touched because it was already running in production, and the principal developer who knew most about it had left the company. P16 said that “most of the, like, actual models were trained before time.” P14 described a “pipeline jungle” that was difficult to maintain:"
    ]
  },
  {
    "input": "O que são métricas de avaliação?",
    "expected_output": "São medidas numéricas que indicam a qualidade do modelo, como accuracy, F1 e RMSE.",
    "actual_output": "Olá!\n\nDe acordo com o contexto, as métricas de avaliação mencionadas são relacionadas ao produto e incluem índices como taxa de clique (click-through rate) ou taxa de perda de usuários (user churn rate). Além disso, também há menção à MAP (Mean Average Precision), que é uma métrica específica para modelo de aprendizado de máquina.\n\nPortanto, as métricas de avaliação são critérios utilizados para avaliar o desempenho de um modelo de aprendizado de máquina em relação ao produto.",
    "retrieval_context": [
      "4.4.4 ML evaluation metrics should be tied to product metrics. Mul- tiple participants stressed the importance of evaluating metrics critical to the product, such as click-through rate or user churn rate, rather than ML-specific metrics alone like MAP (P5, P7, P15, P16, P11, P17, P18, P19). The need to evaluate product-critical metrics stemmed from close collaboration with other stakeholders, such as product managers and business operators. P11 felt that a key reason many ML projects fail is that they don’t measure metrics that will yield the organization value:",
      "Since product-specific metrics are, by definition, different for different ML models, it was important for engineers to treat choos- ing the metrics as an explicit step in their workflow and align with other stakeholders to make sure the right metrics were chosen. For example, P16 said that for every new ML project they work on, their “first task is to figure out, what are customers actually interested in, or what’s the metric that they care about.” P17 said that every model change in production is validated by the product team: “if we can get a statistically significant greater percentage people to subscribe to , then .”",
      "For some organizations, a consequence of tightly coupling eval- uation to product metrics was an additional emphasis on important customers during evaluation (P6, P10). P6 described how, at their company, experimental changes that increased aggregate metrics could sometimes be prevented from going to production:\nThere’s an system to allocate resources for . We have hard-coded rules for mission critical customers. Like at the beginning of Covid, there were hospital that we had to save for.\nParticipants who came from research or academia noted that tying evaluation to the product metrics was a different experience. P6 commented:",
      "4.4.4 ML evaluation metrics should be tied to product metrics. Mul- tiple participants stressed the importance of evaluating metrics critical to the product, such as click-through rate or user churn rate, rather than ML-specific metrics alone like MAP (P5, P7, P15, P16, P11, P17, P18, P19). The need to evaluate product-critical metrics stemmed from close collaboration with other stakeholders, such as product managers and business operators. P11 felt that a key reason many ML projects fail is that they don’t measure metrics that will yield the organization value:",
      "Since product-specific metrics are, by definition, different for different ML models, it was important for engineers to treat choos- ing the metrics as an explicit step in their workflow and align with other stakeholders to make sure the right metrics were chosen. For example, P16 said that for every new ML project they work on, their “first task is to figure out, what are customers actually interested in, or what’s the metric that they care about.” P17 said that every model change in production is validated by the product team: “if we can get a statistically significant greater percentage people to subscribe to , then .”\nFor some organizations, a consequence of tightly coupling eval- uation to product metrics was an additional emphasis on important customers during evaluation (P6, P10). P6 described how, at their company, experimental changes that increased aggregate metrics could sometimes be prevented from going to production:"
    ]
  },
  {
    "input": "O que é um modelo baseline?",
    "expected_output": "É um modelo simples usado como referência para comparar modelos mais complexos.",
    "actual_output": "Olá!\n\nA pergunta é sobre o conceito de modelo baseline. No contexto fornecido, não há uma definição explícita para esse termo. No entanto, podemos relacionar a discussão sobre MLOps (Machine Learning Operations) e o processo de desenvolvimento de modelos.\n\nEm geral, um modelo baseline refere-se ao modelo inicial ou padrão que serve como base para comparações subsequentes. Nesse sentido, é o modelo que você começa com, antes de fazer ajustes ou melhorias.\n\nNo contexto da discussão sobre MLOps, o modelo baseline pode ser visto como o modelo treinado e avaliado no passo 4 (Model training) do processo. Isso significa que o modelo baseline é o modelo inicialmente treinado e avaliado, antes de qualquer ajuste ou refinamento.\n\nPortanto, a resposta para sua pergunta é: Nenhuma informação disponível no contexto.",
    "retrieval_context": [
      "3. Data preparation: The data is prepared for the ML task. This preparation involves data\ncleaning, where you split the data into training, validation, and test sets. You also\napply data transformations and feature engineering to the model that solves the target task. The output of this step are the data splits in the prepared format.\n4. Model training: The data scientist implements different algorithms with the prepared\ndata to train various ML models. In addition, you subject the implemented algorithms to hyperparameter tuning to get the best performing ML model. The output of this\nstep is a trained model.\n5. Model evaluation: The model is evaluated on a holdout test set\n(https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Holdout_dataset) to evaluate",
      "8. Model monitoring: The model predictive performance is monitored to potentially\ninvoke a new iteration in the ML process.\nThe level of automation of these steps defines the maturity of the ML process, which\nreflects the velocity of training new models given new data or training new models given new implementations. The following sections describe three levels of MLOps, starting from\nthe most common level, which involves no automation, up to automating both ML and CI/CD pipelines.\nMLOps level 0: Manual process\nMany teams have data scientists and ML researchers who can build state-of-the-art\nmodels, but their process for building and deploying ML models is entirely manual. This is considered the basic level of maturity, or level 0. The following diagram shows the workflow\nof this process.",
      "I’m gonna start with a model because it means faster . And often, like most of the time em- pirically, it’s gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\nPrior work has also identified the importance of data work .",
      "I’m gonna start with a model because it means faster . And often, like most of the time em- pirically, it’s gonna be something in our data that we can use to kind of push the boundary...obviously it’s not like a dogmatic We Will Never Touch The Model, but it shouldn’t be our first move.\nPrior work has also identified the importance of data work .",
      "high-level flow of control and on key inputs and outputs.\nFigure 3. The MLOps process\nThis is not a waterfall workflow that has to sequentially pass through all the processes. The processes can be\nskipped, or the flow can repeat a given phase or a subsequence of the processes. The diagram shows the following\nflow:\n1. The core activity during this ML development phase is experimentation. As data scientists and ML research-\ners prototype model architectures and training routines, they create labeled datasets, and they use features\nand other reusable ML artifacts that are governed through the data and model management process. The\n8\nprimary output of this process is a formalized training procedure, which includes data preprocessing, model\narchitecture, and model training settings.\n2.\nIf the ML system requires continuous training (repeated retraining of the model), the training procedure is"
    ]
  },
  {
    "input": "O que significa monitorar latência de inferência?",
    "expected_output": "Monitorar latência garante que o modelo responda dentro dos limites necessários para uso real.",
    "actual_output": "Olá!\n\nMonitorar latência de inferência é uma métrica importante para avaliar o desempenho de um modelo após sua implantação. A latência de inferência se refere ao tempo necessário para que o modelo faça uma previsão ou inferência a partir dos dados de entrada.\n\nNo contexto, monitorar latência de inferência é crucial para garantir a estabilidade e eficiência do sistema. Isso ajuda a identificar qualquer problema de desempenho do modelo e detectar erros ou anomalias nas previsões.\n\nAlém disso, a monitoração da latência de inferência permite que você identifique problemas rapidamente e os resolva antes que eles afetem negativamente o desempenho do modelo.",
    "retrieval_context": [
      "low-latency responses is essential.\nMonitoring\nOnce the model is deployed, it is essential to monitor its performance\ncontinuously. Monitoring plays a vital role in identifying any degradation\nin model performance and detecting errors or anomalies in the\npredictions.\nSeveral metrics can be used to evaluate model performance, such as\naccuracy, precision, recall, and F1 score. Additionally, it is crucial to\nmonitor infrastructure-related metrics – like latency, throughput, and\nresource consumption – to guarantee the system’s stability and efficiency.\nBy actively monitoring the model and its surrounding infrastructure, it is\npossible to identify any issues early and swiftly address them. This process\nhelps maintain a high-quality, reliable, and performant model that can",
      "While rudimentary data checks were embedded in most systems, P6 discussed that it was hard to figure out what higher-order data checks to compute:\nMonitoring is both metrics and then a predicate over those metrics that triggers alerts. That second piece doesn’t exist—not because the infrastructure is hard, but because no one knows how to set those predicate values...for a lot of this stuff now, there’s engineering headcount to support a team doing this stuff. This is people’s jobs now; this constant, periodic evaluation of models.\nSome participants discussed using black-box data monitoring services but lamented that their alerts did not prevent failures (P7, P14). P7 said:",
      ". For example, consider the case in which an input signal was previously mis-calibrated. The model consuming it likely ﬁt to these mis-calibrations, and a silent update that corrects the signal will have sudden ramiﬁcations for the model.",
      "The key question is: what to monitor? Testable invariants are not always obvious given that many ML systems are intended to adapt over time. We offer the following starting points.\nPrediction Bias. In a system that is working as intended, it should usually be the case that the distribution of predicted labels is equal to the distribution of observed labels. This is by no means a comprehensive test, as it can be met by a null model that simply predicts average values of label occurrences without regard to the input features. However, it is a surprisingly useful diagnostic, and changes in metrics such as this are often indicative of an issue that requires attention. For example, this method can help to detect cases in which the world behavior suddenly changes, making training distributions drawn from historical data no longer reﬂective of current reality. Slicing prediction bias by various dimensions isolate issues quickly, and can also be used for automated alerting.",
      "While rudimentary data checks were embedded in most systems, P6 discussed that it was hard to figure out what higher-order data checks to compute:\nMonitoring is both metrics and then a predicate over those metrics that triggers alerts. That second piece doesn’t exist—not because the infrastructure is hard, but because no one knows how to set those predicate values...for a lot of this stuff now, there’s engineering headcount to support a team doing this stuff. This is people’s jobs now; this constant, periodic evaluation of models.\nSome participants discussed using black-box data monitoring services but lamented that their alerts did not prevent failures (P7, P14). P7 said:\nWe don’t find those metrics are useful. I guess, what’s the point in tracking these? Sometimes it’s really to cover my ass. If someone asked, how come the performance dropped from X to Y, I could go back in the data and say, there’s a slight shift in the"
    ]
  },
  {
    "input": "O que é validação cruzada?",
    "expected_output": "É uma técnica que divide os dados em múltiplos subconjuntos para avaliar melhor a generalização.",
    "actual_output": "Olá!\n\nAqui no contexto, não há uma definição específica sobre \"validação cruzada\". No entanto, podemos relacionar a discussão sobre modelo de aprendizado automático e sua validação com a ideia de validação cruzada.\n\nA validação cruzada é um conceito que se refere ao processo de avaliar o desempenho de um modelo de aprendizado automático utilizando dados diferentes daqueles utilizados para treinar o modelo. Isso ajuda a garantir que o modelo não esteja apenas memorizando os padrões dos dados de treinamento, mas sim seja capaz de generalizar bem para novos dados.\n\nNo contexto, a discussão sobre validação de modelos e a necessidade de avaliar o desempenho do modelo em diferentes conjuntos de dados pode ser vista como uma forma de aplicar a ideia de validação cruzada.",
    "retrieval_context": [
      "Undeclared Consumers. Oftentimes, a prediction from a machine learning model ma is made widely accessible, either at runtime or by writing to ﬁles or logs that may later be consumed by other systems. Without access controls, some of these consumers may be undeclared, silently using the output of a given model as an input to another system. In more classical software engineering, these issues are referred to as visibility debt .",
      "Data values skews: These skews are significant changes in the statistical properties of data, which means that data patterns are changing, and you need to trigger a retraining of the model to capture these changes.\nModel validation: This step occurs after you successfully train the model given the new data. You evaluate and validate the model before it's promoted to production. This offline model validation step consists of the following.\nProducing evaluation metric values using the trained model on a test dataset to assess the model's predictive quality.\nComparing the evaluation metric values produced by your newly trained model",
      "4.4.1 Validation datasets should be dynamic. Many engineers re- ported processes to analyze live failure modes and update the vali- dation datasets to prevent similar failures from happening again (P1, P2, P5, P6, P8, P11, P15, P16, P17, P18). P1 described this process as a departure from what they had learned in academia: “You have this classic issue where most researchers are evaluat against fixed data sets... most industry methods change their datasets",
      "4.4.2 Validation systems should be standardized. The dynamic na- ture of validation processes makes it hard to effectively maintain versions of such processes, motivating efforts to standardize them. Several participants recalled instances of bugs stemming from in- consistent definitions of successful validation—i.e., where different engineers on their team evaluated models differently, causing un- expected changes to live performance metrics (P1, P3, P4, P5, P6, P7, P17). For instance, P4 lamented that every engineer working on a particular model had a cloned version of the main evaluation notebook, with a few changes",
      "Data values skews: These skews are significant changes in the statistical properties of data, which means that data patterns are changing, and you need to trigger a retraining of the model to capture these changes.\nModel validation: This step occurs after you successfully train the model given the new data. You evaluate and validate the model before it's promoted to production. This offline model validation step consists of the following.\nProducing evaluation metric values using the trained model on a test dataset to assess the model's predictive quality.\nComparing the evaluation metric values produced by your newly trained model\nto the current model, for example, production model, baseline model, or other business-requirement models. You make sure that the new model produces better performance than the current model before promoting it to production."
    ]
  },
  {
    "input": "O que é model drift detection?",
    "expected_output": "É o processo automatizado de identificar mudanças na distribuição ou relação dos dados que afetam a performance do modelo.",
    "actual_output": "Olá!\n\nDe acordo com o contexto, o modelo de detecção de drift (model drift detection) é uma funcionalidade importante no modelo monitoring. Isso significa que a capacidade de monitorar modelos permite detectar mudanças nos dados e conceitos ao longo do tempo, garantindo a qualidade preditiva e a continuidade empresarial.\n\nA detecção de drift é fundamental para evitar problemas como o exemplo citado no contexto, onde um modelo pode ter sido treinado com uma entrada de sinal mal-calibrada e, quando essa entrada for atualizada, o modelo pode sofrer mudanças significativas.\n\nPortanto, a detecção de drift é crucial para garantir que os modelos estejam sempre atualizados e eficazes em produção.",
    "retrieval_context": [
      "Continuous Integration.\nMonitoring identifies model drift over time. Without model monitoring,\nproduction systems are flying blind. By monitoring for model drift the data\nscience team is able to proactively work rather than reactively.\nTesting ensures the accuracy and reliability of models. Validating both\nthe model’s predictions and the data sets used is a fundamental step in\ngreenlighting models for production.\nUse A/B testing to identify best models. A/B testing is sometimes\noverlooked in Machine Learning but is a great way to introduce new\nmodels. Rather than swapping models out straight away you can introduce\nthe new model alongside the old. This weighted approach allows you to\nsee the efficacy of the new model in production before committing to it.\n4. Version Control",
      ". As an anecdote, we asked P17 to give an example of a natural data drift problem their company faced, and they could not think of a good example. P14 also said they don’t have natural data drift problems:",
      "Support canary and shadow deployments.\n\nSupport traffic splitting and A/B tests.\n\nSupport multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production\nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to\nbe investigated and updated. Key functionalities in model monitoring include the following:\nMeasure model efficiency metrics like latency and serving-resource utilization.\nDetect data skews, including schema anomalies and data and concept shifts and drifts.",
      "Support canary and shadow deployments.\n\nSupport traffic splitting and A/B tests.\n\nSupport multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production\nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to\nbe investigated and updated. Key functionalities in model monitoring include the following:\nMeasure model efficiency metrics like latency and serving-resource utilization.\nDetect data skews, including schema anomalies and data and concept shifts and drifts.",
      ". For example, consider the case in which an input signal was previously mis-calibrated. The model consuming it likely ﬁt to these mis-calibrations, and a silent update that corrects the signal will have sudden ramiﬁcations for the model."
    ]
  },
  {
    "input": "Qual a importância de um lineage tracker em MLOps?",
    "expected_output": "Lineage tracking permite rastrear dados, modelos, experimentos e artefatos para auditoria e reprodutibilidade.",
    "actual_output": "Olá!\n\nA importância de um lineage tracker em MLOps é fundamental! Segundo o contexto, um lineage tracker é uma funcionalidade do ML metadata e artifact tracking que fornece traceabilidade e rastreamento da origem dos artefatos de aprendizado de máquina (ML). Isso significa que você pode saber exatamente onde cada artefato veio de e como ele foi gerado, o que é crucial para a reproducibilidade e depuração de tarefas e pipelines complexos.\n\nCom um lineage tracker, você pode rastrear a origem dos dados, modelos treinados, resultados de avaliação e estatísticas, o que ajuda a manter a transparência e a confiabilidade em todo o processo de desenvolvimento de ML. Além disso, isso também facilita a identificação de problemas e a resolução de erros, pois você pode investigar rapidamente a história de cada artefato e entender como ele foi gerado.\n\nPortanto, um lineage tracker é essencial para garantir a reproducibilidade e a confiabilidade em MLOps!",
    "retrieval_context": [
      "artifact tracking capability is foundational to all other MLOps capabilities. Such a capability enables reproducibility\nand debugging of complex ML tasks and pipelines. Key functionalities in ML metadata and artifact tracking include\nthe following:\nProvide traceability and lineage tracking of ML artifacts.",
      "statistics and data schemas, trained models, and evaluation results. ML metadata is the information about these\nartifacts, including their location, types, properties, and associations to experiments and runs. The ML metadata and\nartifact tracking capability is foundational to all other MLOps capabilities. Such a capability enables reproducibility\nand debugging of complex ML tasks and pipelines. Key functionalities in ML metadata and artifact tracking include\nthe following:\nProvide traceability and lineage tracking of ML artifacts.",
      "Support various data modalities, including tabular data, images, and text.\nML data assets can be managed at the entity features level or at the full dataset level. For example, a feature reposi-\ntory might contain an entity called customer, which includes features like age group, postal code, and gender. On the\nother hand, a dataset repository might include a customer churn dataset, which includes features from the customer\nand product entities, as well as purchase- and web-activity event logs.\nML metadata and artifact tracking\nVarious types of ML artifacts are produced in different processes of the MLOps lifecycle, including descriptive\nstatistics and data schemas, trained models, and evaluation results. ML metadata is the information about these\nartifacts, including their location, types, properties, and associations to experiments and runs. The ML metadata and\nartifact tracking capability is foundational to all other MLOps capabilities. Such a capability enables reproducibility",
      "Share and track experimentation and pipeline parameter configurations.\n\nStore, access, investigate, visualize, download, and archive ML artifacts.",
      "Share and track experimentation and pipeline parameter configurations.\n\nStore, access, investigate, visualize, download, and archive ML artifacts."
    ]
  },
  {
    "input": "O que é continuous training (CT)?",
    "expected_output": "CT é a prática de treinar novamente modelos automaticamente com novos dados conforme regras definidas.",
    "actual_output": "Olá!\n\nDe acordo com o contexto, o Continuous Training (CT) é um processo único em MLOps que se preocupa em automaticamente reentrenar e servir os modelos. Isso ocorre quando detecta-se desempenho degradado na versão do modelo atualmente implantada.\n\nEm outras palavras, o CT é uma forma de manter o modelo treinado sempre atualizado e eficaz, retrainando-o automaticamente quando necessário, seja por causa da entrada de novos dados ou pela detecção de desempenho decrescente.",
    "retrieval_context": [
      "architecture, and model training settings.\n2.\nIf the ML system requires continuous training (repeated retraining of the model), the training procedure is\noperationalized as a training pipeline. This requires a CI/CD routine to build, test, and deploy the pipeline to\nthe target execution environment.\n3. The continuous training pipeline is executed repeatedly based on retraining triggers, and it produces a model\nas output. The model is retrained as new data becomes available, or if model performance decay is detected.\nOther training artifacts and metadata that are produced by a training pipeline are also tracked. If the pipeline\nproduces a successful model candidate, that candidate is then tracked by the model management process\nas a registered model.",
      "architecture, and model training settings.\n2.\nIf the ML system requires continuous training (repeated retraining of the model), the training procedure is\noperationalized as a training pipeline. This requires a CI/CD routine to build, test, and deploy the pipeline to\nthe target execution environment.\n3. The continuous training pipeline is executed repeatedly based on retraining triggers, and it produces a model\nas output. The model is retrained as new data becomes available, or if model performance decay is detected.\nOther training artifacts and metadata that are produced by a training pipeline are also tracked. If the pipeline\nproduces a successful model candidate, that candidate is then tracked by the model management process\nas a registered model.\n4. The registered model is annotated, reviewed, and approved for release and is then deployed to a production\nenvironment. This process might be relatively opaque if you are using a no-code solution, or it can involve",
      "testing, integration testing, and continuous delivery of the software module or the package. However, in ML, there are a few notable differences:\nCI is no longer only about testing and validating code and components, but also\ntesting and validating data, data schemas, and models.\nCD is no longer about a single software package or a service, but a system (an ML training pipeline) that should automatically deploy another service (model prediction\nservice).\nCT is a new property, unique to ML systems, that's concerned with automatically retraining and serving the models.\nThe following section discusses the typical steps for training and evaluating an ML model\nto serve as a prediction service.\nData science steps for ML\nIn any ML project, after you define the business use case and establish the success criteria, the process of delivering an ML model to production involves the following steps. These\nsteps can be completed manually or can be completed by an automatic pipeline.",
      "service).\nCT is a new property, unique to ML systems, that's concerned with automatically retraining and serving the models.\nThe following section discusses the typical steps for training and evaluating an ML model\nto serve as a prediction service.\nData science steps for ML\nIn any ML project, after you define the business use case and establish the success criteria, the process of delivering an ML model to production involves the following steps. These\nsteps can be completed manually or can be completed by an automatic pipeline.\n1. Data extraction: You select and integrate the relevant data from various data sources\nfor the ML task.\n2. Data analysis: You perform exploratory data analysis (EDA)",
      "In MLOps, data is a necessary input for developing the machine learning model. But in DevOps, data is an output of the program, not an input.\nIn MLOPs, the model must be validated continuously in production for performance deterioration caused by new data over time. The software system does not deteriorate in DevOps; it is merely monitored for health maintenance purposes.\nConcepts such as model training, model testing, and validation are all unique to MLOps and irrelevant in the conventional software realm of DevOps. Moreover, the training model tends to be compute-intensive, hence requiring (typically) the use of powerful GPUs.\nMLOps requires Continuous Training (CT), a process that automatically identifies scenarios that require a particular model to be retrained and re-deployed due to performance degradation in the currently deployed version of the model."
    ]
  },
  {
    "input": "Como funciona um sistema de AB testing para modelos?",
    "expected_output": "O sistema divide tráfego entre modelos diferentes para comparar performance estatisticamente.",
    "actual_output": "Olá!\n\nAqui vai uma resposta clara e informativa sobre como funciona um sistema de A/B testing para modelos:\n\nO A/B testing é uma técnica utilizada para avaliar a eficácia de novos modelos em produção, antes de substituir os modelos antigos. Nesse sentido, o sistema de A/B testing para modelos consiste em introduzir o novo modelo ao lado do modelo antigo, com pesos definidos (ou seja, uma abordagem ponderada). Isso permite que você veja como o novo modelo se comporta em produção antes de comprometer-se com ele.\n\nNesse processo, um porcento determinado da tráfego é direcionado para o novo modelo, enquanto o restante continua a ser direcionado ao modelo antigo. Isso permite que você compare as performances dos dois modelos e escolha o melhor.\n\nEssa abordagem é especialmente útil quando você está trabalhando com modelos de aprendizado automático (ML) em produção, pois permite que você avalie a eficácia do novo modelo antes de substituir o modelo antigo. Além disso, o A/B testing ajuda a identificar os melhores modelos para uso em produção.\n\nEspero que isso tenha ajudado!",
    "retrieval_context": [
      "Continuous Integration.\nMonitoring identifies model drift over time. Without model monitoring,\nproduction systems are flying blind. By monitoring for model drift the data\nscience team is able to proactively work rather than reactively.\nTesting ensures the accuracy and reliability of models. Validating both\nthe model’s predictions and the data sets used is a fundamental step in\ngreenlighting models for production.\nUse A/B testing to identify best models. A/B testing is sometimes\noverlooked in Machine Learning but is a great way to introduce new\nmodels. Rather than swapping models out straight away you can introduce\nthe new model alongside the old. This weighted approach allows you to\nsee the efficacy of the new model in production before committing to it.\n4. Version Control",
      "Enable on-demand use of ML accelerators.\nAllow efficient hyperparameter tuning and target optimization at scale.\n\nIdeally, provide built-in automated ML (AutoML) functionality, including automated feature selection and engi-\nneering as well as automated model architecture search and selection.\nModel evaluation\nThe model evaluation capability lets you assess the effectiveness of your model, interactively during experimentation\nand automatically in production. Key functionalities in model evaluation include the following:\nPerform batch scoring of your models on evaluation datasets at scale.\nCompute pre-defined or custom evaluation metrics for your model on different slices of the data.",
      "Support canary and shadow deployments.\n\nSupport traffic splitting and A/B tests.\n\nSupport multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production\nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to\nbe investigated and updated. Key functionalities in model monitoring include the following:\nMeasure model efficiency metrics like latency and serving-resource utilization.\nDetect data skews, including schema anomalies and data and concept shifts and drifts.",
      "Support canary and shadow deployments.\n\nSupport traffic splitting and A/B tests.\n\nSupport multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production\nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to\nbe investigated and updated. Key functionalities in model monitoring include the following:\nMeasure model efficiency metrics like latency and serving-resource utilization.\nDetect data skews, including schema anomalies and data and concept shifts and drifts.",
      "The model architecture and pretrained modules that were used.\nHyperparameters, including trials of automated hyperparameter tuning and model selection."
    ]
  },
  {
    "input": "O que é um modelo champion vs challenger?",
    "expected_output": "O modelo champion é o atual em produção, enquanto challengers são avaliados como potenciais substitutos.",
    "actual_output": "Olá!\n\nNo contexto fornecido, não há uma definição específica para \"champion\" e \"challenger\". No entanto, podemos relacionar esses termos ao processo de treinamento e avaliação de modelos de aprendizado automático.\n\nEm geral, um modelo champion é o melhor modelo treinado e avaliado em relação a um conjunto de métricas específicas, como precisão ou erro quadrático médio. Já um modelo challenger é um modelo que desafia o modelo champion, tentando superá-lo em termos de performance.\n\nNo contexto do treinamento de modelos de aprendizado automático, os engenheiros de ML podem criar vários modelos e avaliá-los para encontrar o melhor modelo (o champion) para uma tarefa específica. Em seguida, eles podem criar um modelo challenger para testar se é possível melhorar a performance do modelo champion.\n\nPortanto, embora não haja uma definição explícita para \"champion\" e \"challenger\" no contexto fornecido, podemos relacioná-los ao processo de treinamento e avaliação de modelos de aprendizado automático.",
    "retrieval_context": [
      "Undeclared Consumers. Oftentimes, a prediction from a machine learning model ma is made widely accessible, either at runtime or by writing to ﬁles or logs that may later be consumed by other systems. Without access controls, some of these consumers may be undeclared, silently using the output of a given model as an input to another system. In more classical software engineering, these issues are referred to as visibility debt .",
      "Model training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually\nadjusted, and its performance is evaluated against a validation dataset to\nfine-tune its accuracy and effectiveness.\nSeveral techniques can be applied during the model training phase,\nincluding hyperparameter optimisation, cross-validation, and\nregularisation. Utilising the right combination of these methods helps\nreduce the risk of model overfitting and improve its generalisation\ncapabilities.\nModel Deployment\nAfter the model has been trained and its performance validated, it is time\nfor model deployment. Model deployment is the process of integrating\nthe model into the production environment, making it accessible to end-",
      ". Feature Engineering and Model Experimentation. ML engi- neers typically focus on improving ML performance, measured via metrics such as accuracy or mean-squared-error. Experiments can be data-driven or model-driven; for example, an engineer can create a new feature or change the model architecture from tree-based to neural network-based. Model Evaluation and Deployment. A model is typically eval- uated by computing a metric (e.g., accuracy) over a collection of labeled data points hidden at training time, or a validation dataset, to see if its performance is better than what the currently-running production model achieved during its evaluation phase",
      "The model architecture and pretrained modules that were used.\nHyperparameters, including trials of automated hyperparameter tuning and model selection.",
      "The model architecture and pretrained modules that were used.\nHyperparameters, including trials of automated hyperparameter tuning and model selection."
    ]
  }
]